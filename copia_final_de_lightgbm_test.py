# -*- coding: utf-8 -*-
"""lightGbm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lzXR-i34vRG_8w_wp4wC3QdpVxgNGOPh
"""

!pip install mlforecast
!pip install window_ops

###### Feature Engineering ######
import pandas as pd
import numpy as np
from window_ops.rolling import rolling_mean, rolling_max, rolling_min

###### Load the API  ######
import requests
import gzip
import io

###### Forecasting ######
from mlforecast import MLForecast
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from xgboost import XGBRegressor
import lightgbm as lgb



###### Visualize ######
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import numpy as np
from window_ops.rolling import rolling_mean, rolling_max, rolling_min
from sklearn.base import BaseEstimator, TransformerMixin # Import BaseEstimator and TransformerMixin
from sklearn.preprocessing import LabelEncoder # Import LabelEncoder

###### Load the API  ######
import requests
import gzip
import io

###### Forecasting ######
from mlforecast import MLForecast
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from xgboost import XGBRegressor
import lightgbm as lgb


###### Visualize ######
import matplotlib.pyplot as plt
import seaborn as sns

"""armamos funciones basicas"""

import numpy as np
import pandas as pd
import re
from sklearn.metrics import mean_absolute_error
import lightgbm as lgb
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import LabelEncoder
import requests
import gzip
import io
import matplotlib.pyplot as plt
import seaborn as sns


def calculate_error(forecast: np.ndarray, actual: np.ndarray) -> float:
    """
    Calcula el Total Forecast Error:

        ∑ |actual_i - forecast_i|
        -------------------------
           ∑ actual_i

    Parámetros
    ----------
    forecast : np.ndarray
        Array de valores pronosticados (ForecastSales).
    actual : np.ndarray
        Array de valores reales (ActualSales).

    Devuelve
    -------
    float
        El error total de pronóstico.
    """
    # Asegurarnos de que sean floats para evitar divisiones enteras
    forecast = forecast.astype(float)
    actual   = actual.astype(float)

    # Numerador: suma de errores absolutos
    numerador = np.sum(np.abs(actual - forecast))
    # Denominador: suma de ventas reales
    denominador = np.sum(actual)

    # Evitar división por cero
    if denominador == 0:
        raise ValueError("La suma de actual (denominador) es cero.")

    return numerador / denominador



def cargar_dataset(url, sep='\t', header='infer'):
    """
    Lee un archivo CSV comprimido con gzip desde una URL y lo carga en un DataFrame de Pandas.

    :param url: String con la URL del archivo .gz.
    :param sep: String delimitador para separar campos en el archivo CSV.
    :param header: Parámetro para determinar la fila que debe usarse como nombre de las columnas.
    :return: DataFrame de Pandas con los datos del archivo CSV.
    """
    # Realizar la petición HTTP para obtener el contenido del archivo
    response = requests.get(url)

    # Asegurarse de que la petición fue exitosa
    if response.status_code == 200:
        # Descomprimir el contenido en memoria y leer directamente en un DataFrame
        with io.BytesIO(response.content) as bytes_io:
            with gzip.open(bytes_io, 'rt') as read_file:
                df = pd.read_csv(read_file, sep=sep, header=header)
        return df
    else:
        raise IOError(f"Error al descargar el archivo: {response.status_code}")

def read_csv_from_url(url, sep='\t', header='infer'):
    response = requests.get(url)
    if response.status_code == 200:
        df = pd.read_csv(io.StringIO(response.content.decode('utf-8')), sep=sep, header=header)
        return df
    else:
        raise IOError(f"Error al descargar el archivo: {response.status_code}")

def plotear_grafico(df):
    # Convertir el 'periodo' a formato de fecha para mejor visualización en el gráfico
    df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')

    # Crear nuevamente el gráfico de línea con Seaborn
    plt.figure(figsize=(14, 7))
    line_plot = sns.lineplot(data=df, x='periodo', y='tn', marker='o', color='blue', linewidth=2.5)

    # Títulos y etiquetas
    line_plot.set_title('Evolución de Toneladas a lo largo del Tiempo', fontsize=16)
    line_plot.set_xlabel('Periodo', fontsize=14)
    line_plot.set_ylabel('Toneladas (tn)', fontsize=14)

    # Establecer el formato del eje x para mostrar todos los periodos
    line_plot.xaxis.set_major_locator(plt.MaxNLocator(len(df['periodo'])))
    line_plot.set_xticklabels(df['periodo'].dt.strftime('%Y-%m'), rotation=45, ha='right')

    # Mostrar el gráfico
    plt.tight_layout()
    plt.show()

def completar_periodos(df, fecha_inicio="2017-01-01", fecha_fin="2020-02-01"):
    """
    Devuelve un DataFrame con todas las combinaciones product_id × mes
    entre fecha_inicio y fecha_fin. Los campos que varían mes a mes se
    dejan en NaN (el modelo los completará después).
    """
    # 1) calendario mensual (primer día de mes)
    meses = pd.date_range(start=fecha_inicio, end=fecha_fin, freq="MS")

    # 2) producto cartesiano product_id × meses
    full_idx = pd.MultiIndex.from_product(
        [df["product_id"].unique(), meses],
        names=["product_id", "periodo"]
    )
    df_full = (df
               .set_index(["product_id", "periodo"])
               .reindex(full_idx)
               .reset_index())

    # 3) columnas estáticas que SÍ podemos rellenar
    cols_static = ["plan_precios_cuidados", "cat1", "cat2", "cat3", "brand", "sku_size"]
    df_full.sort_values(["product_id", "periodo"], inplace=True)
    df_full[cols_static] = (
        df_full.groupby("product_id")[cols_static]
               .ffill()
               .bfill()
    )

    # tn, cust_request_tn y futuros lags quedan en NaN
    return df_full



def generar_delta_lags_optimizado(df, prefijo_lag='lag_', prefijo_delta_lag='delta_lag_'):
    """
    Genera delta lags de manera más eficiente para una columna específica de un DataFrame de pandas.

    :param df: DataFrame original que ya contiene las columnas de lags.
    :param prefijo_lag: Prefijo utilizado para las columnas de lags en el DataFrame.
    :param prefijo_delta_lag: Prefijo utilizado para las nuevas columnas de delta lags.
    :return: DataFrame con las columnas de delta lags añadidas.
    """

    # Identificar todas las columnas de lags en el DataFrame
    columnas_lags = [col for col in df.columns if col.startswith(prefijo_lag)]

    # Generar las columnas de delta lags
    for i in range(len(columnas_lags) - 1):
        col_lag_actual = columnas_lags[i]
        col_lag_siguiente = columnas_lags[i + 1]
        df[prefijo_delta_lag + str(i + 1)] = df[col_lag_actual] - df[col_lag_siguiente]

    return df

# Re-creating the optimized lag generation function after the reset
def generar_lags_optimizado(df, nombre_col_original='tn', cant_lags=12):
    """
    Genera lags de manera más eficiente para una columna específica de un DataFrame de pandas.

    :param df: DataFrame original.
    :param nombre_col_original: Nombre de la columna sobre la cual se quieren calcular los lags.
    :param cant_lags: Cantidad de lags a generar.
    :return: DataFrame con las columnas de lags añadidas.
    """

    # Obtener una lista de productos únicos
    productos_unicos = df['product_id'].unique()

    # Preparar un contenedor para los DataFrames de lags
    lags_list = []

    # Iterar sobre cada producto único
    for producto in productos_unicos:
        # Filtrar el DataFrame por producto
        df_producto = df[df['product_id'] == producto].copy()

        # Generar lags para la columna deseada
        for lag in range(1, cant_lags + 1):
            df_producto[f'lag_{lag}'] = df_producto[nombre_col_original].shift(lag)

        # Añadir el DataFrame de lags a la lista
        lags_list.append(df_producto)

    # Concatenar todos los DataFrames de lags
    df_con_lags = pd.concat(lags_list).sort_index()

    return df_con_lags


def generar_lags_optimizadov2(df, col='tn', n_lags=12, prefijo='lag'):
    """
    Añade columnas lag_1 … lag_n_lags para `col`, por product_id,
    de forma vectorizada (sin bucles externos costosos).

    Parameters
    ----------
    df : pandas.DataFrame
    col : str
        Nombre de la columna sobre la que quieres los lags.
    n_lags : int
        Número de lags a generar.
    prefijo : str
        Prefijo de las nuevas columnas (lag, stock_t, etc.).

    Returns
    -------
    pandas.DataFrame
        El DataFrame original + columnas de lags.
    """
    out = df.copy()
    for k in range(1, n_lags + 1):
        out[f"{prefijo}_{k}"] = (
            out.groupby("product_id")[col].transform(lambda s: s.shift(k))
        )
    return out



def generar_medias_optimizado(df, nombre_col_original='tn', lista_medias_target=[2,3,4,6,10], prefijo_col_media='media'):
    """
    Genera las medias móviles de los últimos N meses de manera más eficiente para una columna específica de un DataFrame de pandas.

    :param df: DataFrame original.
    :param nombre_col_original: Nombre de la columna sobre la cual se quieren calcular las medias móviles.
    :param lista_medias_target: Lista de enteros que representan el número de meses para calcular la media móvil.
    :param prefijo_col_media: Prefijo para las nuevas columnas de medias móviles.
    :return: DataFrame con las columnas de medias móviles añadidas.
    """

    # Obtener una lista de productos únicos
    productos_unicos = df['product_id'].unique()

    # Preparar un contenedor para los DataFrames de medias móviles
    medias_list = []

    # Iterar sobre cada producto único
    for producto in productos_unicos:
        # Filtrar el DataFrame por producto
        df_producto = df[df['product_id'] == producto].copy()

        # Generar medias móviles para cada ventana especificada en lista_medias_target
        for ventana in lista_medias_target:
            col_media = f'{nombre_col_original}_{prefijo_col_media}_{ventana}'
            df_producto[col_media] = df_producto[nombre_col_original].rolling(window=ventana).mean().shift()

        # Añadir el DataFrame de medias móviles a la lista
        medias_list.append(df_producto)

    # Concatenar todos los DataFrames de medias móviles
    df_con_medias = pd.concat(medias_list).sort_index()

    return df_con_medias


class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.encoders = {}

    def fit(self, X, y=None):
        for column in X.columns:
            self.encoders[column] = LabelEncoder().fit(X[column])
        return self

    def transform(self, X):
        X_encoded = X.copy()
        for column in X.columns:
            X_encoded[column] = self.encoders[column].transform(X[column])
        return X_encoded

    def inverse_transform(self, X):
        X_decoded = X.copy()
        for column in X.columns:
            X_decoded[column] = self.encoders[column].inverse_transform(X[column])
        return X_decoded


# ────────────────────────────────────────────────────────────────
# 1️⃣  LAGS  (prefijo inteligente)
# ────────────────────────────────────────────────────────────────
def generar_lags_optimizadov2(df, col='tn', n_lags=12, prefijo='lag'):
    """
    Crea columnas lag_k para `col` por product_id.
    ✔ No añade subrayado duplicado:
        prefijo = 'lag'      → lag_1
        prefijo = 'stock_t'  → stock_t1
    """
    sep = "_" if not prefijo.endswith(("_", "t")) else ""   # evita stock_t_1
    out = df.copy()
    for k in range(1, n_lags + 1):
        out[f"{prefijo}{sep}{k}"] = (
            out.groupby("product_id")[col].transform(lambda s: s.shift(k))
        )
    return out

# ────────────────────────────────────────────────────────────────
# 2️⃣  DELTA-LAGS  (regex robusto)
# ────────────────────────────────────────────────────────────────
def generar_delta_lags_optimizado_v2(df, base_prefijo="lag_", prefijo_out="delta_lag_"):
    """
    Delta_k = lag_k − lag_{k+1}  (funciona con 'lag_1' o 'stock_t1').
    """
    out = df.copy()
    cols = sorted(
        [c for c in out.columns if c.startswith(base_prefijo)],
        key=lambda x: int(re.search(r'(\d+)$', x).group(1))
    )
    for i in range(len(cols) - 1):
        k = int(re.search(r'(\d+)$', cols[i]).group(1))
        out[f"{prefijo_out}{k}"] = out[cols[i]] - out[cols[i + 1]]
    return out

# ────────────────────────────────────────────────────────────────
# 3️⃣  MEDIAS  (vectorizado)
# ────────────────────────────────────────────────────────────────
def generar_medias_optimizado_v2(df, col, ventanas=(2,3,4,6,10), prefijo="media"):
    """
    media_k = mean(col, window=k) desplazado 1 periodo.
    """
    out = df.copy()
    g = out.groupby("product_id")[col]
    for w in ventanas:
        out[f"{col}_{prefijo}_{w}"] = (
            g.rolling(window=w, min_periods=1).mean()
              .shift()
              .reset_index(level=0, drop=True)
        )
    return out

# ──────────────────────────────────────────────────────────────
# Función utilitaria
# ──────────────────────────────────────────────────────────────
def eval_producto(
    model,
    df_test,                # DataFrame de diciembre
    product_ids,            # int o lista de ints
    id_col="product_id",
    target_col="tn",        # en log1p
    num_iter=None
):
    """
    Devuelve DataFrame con métricas MAE / MAPE para uno o varios productos.
    """
    if isinstance(product_ids, int):
        product_ids = [product_ids]

    # 1) prepara X_test y predicciones en log
    X_test = prep_strict(df_test)
    y_pred_log = model.predict(X_test, num_iteration=num_iter)

    tmp = df_test.copy()
    tmp["y_pred_log"] = y_pred_log

    # 2) filtra productos seleccionados
    mask = tmp[id_col].isin(product_ids)
    pe   = tmp[mask].copy()

    # 3) pasa a escala real
    pe["y_true_lin"] = np.expm1(pe[target_col])
    pe["y_pred_lin"] = np.expm1(pe["y_pred_log"])
    pe["abs_err"] = np.abs(pe["y_true_lin"] - pe["y_pred_lin"])
    pe["ape"] = np.where(
        pe["y_true_lin"] != 0,
        pe["abs_err"] / pe["y_true_lin"],
        np.nan
    )

    # 4) métricas agregadas por producto
    resumen = (
        pe.groupby(id_col)
          .agg(
              MAE_lin  = ("abs_err", "mean"),
              MAPE_lin = ("ape",    "mean"),
              N_obs    = ("abs_err","size")
          )
          .reset_index()
    )

    return resumen, pe



# ------------------------------------------------------------------
# 3) ENTRENAMIENTO  (con pesos WMAPE)
# ------------------------------------------------------------------
def train_lgb(X_train, y_train, X_valid, y_valid, params,
              num_rounds=3000, early_stop=100, name='model'):

    # Pesos = toneladas reales  (aproxima WMAPE)
    w_train = np.expm1(y_train) if USE_LOG_TARGET else y_train
    w_valid = np.expm1(y_valid) if USE_LOG_TARGET else y_valid

    dtrain = lgb.Dataset(X_train, y_train,
                         weight=w_train,
                         categorical_feature=cat_feats)
    dvalid = lgb.Dataset(X_valid, y_valid,
                         weight=w_valid,
                         categorical_feature=cat_feats)

    booster = lgb.train(
        params,
        dtrain,
        num_boost_round=num_rounds,
        valid_sets=[dvalid],
        valid_names=['valid'],
        callbacks=[
            lgb.early_stopping(early_stop, verbose=False),
            lgb.log_evaluation(200)
        ]
    )
    mae_val = mean_absolute_error(
        y_valid, booster.predict(X_valid, num_iteration=booster.best_iteration)
    )
    print(f"▶ {name} | MAE valid (log): {mae_val:.4f} | best_iter: {booster.best_iteration}")
    return booster

# ------------------------------------------------------------------
# Métrica personalizada
# ------------------------------------------------------------------
def mape_safe(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:
    mask = y_true != 0
    if not np.any(mask):
        return np.nan
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / (y_true[mask] + eps)))

def wmape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:
    return np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + eps)


# Métrica WMAPE para LightGBM
def lgb_wmape_eval(y_pred, dataset):
    # y_pred y y_train están en log → pasamos a toneladas
    y_pred_tn = np.expm1(y_pred) if USE_LOG_TARGET else y_pred
    y_true_tn = np.expm1(dataset.get_label()) if USE_LOG_TARGET else dataset.get_label()

    # si usas pesos = tn reales, tómalos de ahí; si no, usa y_true_tn
    weight = dataset.get_weight()
    if weight is None:
        weight = y_true_tn

    wmape = np.sum(np.abs(y_true_tn - y_pred_tn)) / (np.sum(weight) + 1e-8)
    return "WMAPE", wmape, False        # False → cuanto más bajo, mejor

def feature_pipeline(df: pd.DataFrame) -> pd.DataFrame:
    """
    Recibe un df que YA incluye stock_t0,
    y devuelve el df con todas las features.
    """
    df = df.copy()

    # 1) Calendar
    df["periodo"] = pd.to_datetime(df["periodo"]) # Ensure periodo is datetime
    df["month"]     = df["periodo"].dt.month.astype("int8")
    df["quarter"]   = df["periodo"].dt.quarter.astype("int8")
    df["month_sin"] = np.sin(2*np.pi*df["month"]/12)
    df["month_cos"] = np.cos(2*np.pi*df["month"]/12)

    # 2) Asegura stock_t0 (si no está, lo crea con NaN)
    if "stock_t0" not in df.columns:
        df["stock_t0"] = np.nan

    # 3) Lags de tn (1–24)
    df = generar_lags_optimizadov2(df, col="tn", n_lags=24)

    # 4) Deltas y medias sobre tn
    df = generar_delta_lags_optimizado_v2(df)
    df = generar_medias_optimizado_v2(df, col="tn", ventanas=(2,3,4,6,10))

    # 5) Features de stock
    # Ensure 'lag_1' is created before accessing it
    df["flag_stock_disp"] = (~df["stock_t0"].isna()).astype("int8")
    df["ratio_tn_stock"]  = (
        df["lag_1"] / (df["stock_t0"] + 1e-3) # Access 'lag_1' after it's generated
    ).where(df["flag_stock_disp"] == 1, np.nan)

    # 6) YoY (lag 12, delta_yoy, ratio_yoy)
    df = df.sort_values(["product_id","periodo"])

    # Ensure 'lag_1' and 'lag_13' are created before accessing them
    df["delta_yoy"] = df["lag_1"] - df["lag_13"] # Access 'lag_1' and 'lag_13' after they are generated
    df["ratio_yoy"] = df["lag_1"] / (df["lag_13"] + 1e-6) # Access 'lag_1' and 'lag_13' after they are generated

    return df



# URL del archivo .gz
url_tb_sellout_02 = 'https://storage.googleapis.com/open-courses/austral2025-af91/labo3v/sell-in.txt.gz'
url_tb_stocks_02 = 'https://storage.googleapis.com/open-courses/austral2025-af91/labo3v/tb_stocks.txt'
url_tb_productos_02 = 'https://storage.googleapis.com/open-courses/austral2025-af91/labo3v/tb_productos.txt'





# Uso de la función
df_tb_sellout_02 = cargar_dataset(url_tb_sellout_02)
df_tb_stocks_02= read_csv_from_url(url_tb_stocks_02)
df_tb_productos_02= read_csv_from_url(url_tb_productos_02)

df_tb_sellout_02.head(50)

productospredecir=pd.read_csv("/content/sample_data/product_id_apredecir201912.txt",sep="\t",header=0)

df_tb_sellout_02['periodo'].unique()

# Juntamos los datasets Sellout con el maestro de productos
df_sellout_with_cat = pd.merge(df_tb_sellout_02, df_tb_productos_02[['product_id', 'cat1', 'cat2', 'cat3', 'brand', 'sku_size']], on='product_id', how='left')
# Agrupamos por categorías de productos
df_aggregated = df_sellout_with_cat.groupby(['periodo', 'product_id', 'plan_precios_cuidados',
                                             'cat1', 'cat2', 'cat3', 'brand', 'sku_size']).agg({'cust_request_tn': 'sum','tn': 'sum'}).reset_index()
# 1) Asegúrate de que ambas columnas product_id sean del mismo tipo
df_aggregated["product_id"]      = df_aggregated["product_id"].astype(int)
productospredecir["product_id"] = productospredecir["product_id"].astype(int)

# 2) El join: basta con quedarnos con los IDs que aparecen en la lista
df_filtrado = (
    df_aggregated.merge(
        productospredecir[["product_id"]].drop_duplicates(),  # solo la llave
        on="product_id",
        how="inner"                                           # ← inner join
    )
)

print("Filas antes :", len(df_aggregated))
print("Filas después:", len(df_filtrado))

df = df_filtrado.copy()                        # tu tabla de origen
df["periodo"] = pd.to_datetime(df["periodo"].astype(str), format="%Y%m")
df_full=df

"""### Feature engineering - se transforma a log y se hace primero los lags y deltas sobre las variables crudas como vienen

**Armamos un product id ficticio para correr las escalas ahora lo ignoramos**
"""

df_subset=df_full

# Crear un nuevo DataFrame con los mismos periodos que df_subset
periodos = df_subset['periodo'].unique()  # Extraer los periodos únicos
datos_ficticios = {
    'periodo': periodos,
    'product_id': 99999,  # ID del producto ficticio
    'plan_precios_cuidados': 0.0,
    'cat1': 'HC',
    'cat2': 'Artificial',
    'cat3': 'Artificial',
    'brand': 'Producto_artificial',
    'sku_size': 3000.0,
    # Inicializamos cust_request_tn y tn con ceros, los actualizaremos luego
    'cust_request_tn': [0] * len(periodos),
    'tn': [0] * len(periodos)
}

df_ficticio = pd.DataFrame(datos_ficticios)

# Ahora, actualizamos 'cust_request_tn' y 'tn' con 10 veces los valores del producto 20001
# Asumimos que 'product_id' es único por periodo en df_subset
for periodo in periodos:
    # Encuentra los valores para el producto 20001 en el periodo actual
    valores_producto = df_subset[(df_subset['periodo'] == periodo) & (df_subset['product_id'] == 20001)]
    if not valores_producto.empty:
        df_ficticio.loc[df_ficticio['periodo'] == periodo, 'cust_request_tn'] = valores_producto['cust_request_tn'].values[0] * 3
        df_ficticio.loc[df_ficticio['periodo'] == periodo, 'tn'] = valores_producto['tn'].values[0] * 3

# Concatenar el DataFrame ficticio con el original
df_subset_con_ficticio = pd.concat([df_subset, df_ficticio], ignore_index=True)

"""***aplicamos Lags a todo , stocks - tn - y le hacemos tambien promedios y delta lags, lo que paso es que cuando meti el stock , mejoro pero despues empeor, pueden ser tres cos"""

df_final=df_full

# ───────────────────────────────────────────────────────────
# 🔖 FEATURE: Antigüedad en meses desde primera aparición
# ───────────────────────────────────────────────────────────
# Primero: calculamos la fecha mínima (primera aparición)
fecha_inicio = df_final.groupby('product_id')['periodo'].min().reset_index()
fecha_inicio.columns = ['product_id', 'fecha_primera_aparicion']

# Añadimos esta fecha original al DataFrame principal
df_final = df_final.merge(fecha_inicio, on='product_id', how='left')

# Creamos la variable "antiguedad_meses"
df_final['antiguedad_meses'] = (
    (df_final['periodo'].dt.year - df_final['fecha_primera_aparicion'].dt.year) * 12 +
    (df_final['periodo'].dt.month - df_final['fecha_primera_aparicion'].dt.month)
).astype(int)

# Verificamos rápido
print(df_final[['product_id', 'periodo', 'fecha_primera_aparicion', 'antiguedad_meses']].head(10))

df_final.drop(columns=['fecha_primera_aparicion'], inplace=True)

df_final=df_final


# ------------------------------------------------------------------
# 0)  BASE: ventas lineales
# ------------------------------------------------------------------
feats = df_final.copy()
feats["periodo"] = pd.to_datetime(feats["periodo"], format="%Y%m")

# ------------------------------------------------------------------
# 1) MERGE con stock desplazado +1 mes (stock_t0)
# ------------------------------------------------------------------
stock = (
    df_tb_stocks_02.copy()
        .assign(periodo=lambda d:
                pd.to_datetime(d["periodo"].astype(str), format="%Y%m")
                + pd.offsets.MonthEnd(0)      # 201810 → 2018-10-31
                + pd.offsets.MonthBegin(1))   #        → 2018-11-01
        .rename(columns={"stock_final": "stock_t0"})
)

featsandStock = feats.merge(
    stock[["product_id", "periodo", "stock_t0"]],
    on=["product_id", "periodo"],
    how="left"
)

featsandStock.columns

# ────────────────────────────────────────────────────
# 1) Generas todas las features de una sola vez
# ────────────────────────────────────────────────────
featspipelined = feature_pipeline(featsandStock)   # raw_df incluye periodo, product_id, tn, etc.

# ────────────────────────────────────────────────────
# 2) Target y sample weights
# ────────────────────────────────────────────────────
featspipelined["tn_log"] = np.log1p(featspipelined["tn"])
y = featspipelined["tn_log"]
weights = np.expm1(y)

# ────────────────────────────────────────────────────
# 3) Filtrado histórico de entrenamiento
# ────────────────────────────────────────────────────
mask = featspipelined["periodo"].between("2017-01-01","2019-12-01")
readyforModel = featspipelined.loc[mask].copy()

readyforModel.tail()

"""***Modelo listo para correr***

***Vista Resumida***
"""

# --- Info y resumen de readyforModel -----------------------------------
import pandas as pd
import numpy as np
import io, sys

# A) info() en texto
buf = io.StringIO()
readyforModel.info(buf=buf)
print(">>>  readyforModel.info()")
print(buf.getvalue())

# B) NAs por columna, ordenados
print("\n>>>  Top 30 columnas con más NAs")
na = readyforModel.isna().sum().sort_values(ascending=False)
display(na.head(30))

# C) Estadísticos básicos de las 15 primeras numéricas
print("\n>>>  describe() primeras numéricas")
display(readyforModel.select_dtypes(include=[np.number]).iloc[:, :15].describe().T)

# D) Contar cuántas columnas usan tn_t (terminan en '_t0')
cols_t0 = [c for c in readyforModel.columns if '_t' in c and c.endswith('0')]
print(f"\n>>>  Columnas basadas en tn_t0: {len(cols_t0)}")
print(cols_t0[:15])   # muestra las primeras 15

"""# Armamos Multiples Modelos con diferentes complejidades

## Modelo 1 lightGbm clasico - Sin search de hiperparametros
"""

readyforModel.columns

"""Modelo 1- clasico"""

DROP_COLS_NOW = [
    'tn',       # target
    'tn_log',   # model target
    'periodo',  # <— must drop the datetime
    'cust_request_tn'
    #'ratio_tn_stock',
    #'flag_stock_disp',

    #'stock_t0'
]
cat_feats = ['cat1','cat2','cat3','brand','month','quarter']

"""Empezamos a correr el modelo

Modelo 1-LightGbm base
"""

# ------------------------------------------------------------------
# CONFIGURACIÓN
# ------------------------------------------------------------------
import pandas as pd, numpy as np, lightgbm as lgb
from sklearn.metrics import mean_absolute_error

# ------------------------------------------------------------------
# CONFIGURACIÓN
# ------------------------------------------------------------------

DROP_COLS_NOW = [
    'tn',       # target
    'tn_log',   # model target
    'periodo',  # <— must drop the datetime
    'cust_request_tn'
    #'ratio_tn_stock',
    #'flag_stock_disp',

    #'stock_t0'
]
cat_feats = ['cat1','cat2','cat3','brand','month','quarter']


# ------------------------------------------------------------------
# 1) SPLITS
# ------------------------------------------------------------------
train_oct = readyforModel[readyforModel['periodo'] <= '2019-10-01'].copy()
valid_nov = readyforModel[readyforModel['periodo'] == '2019-11-01'].copy()
test_dec  = readyforModel[readyforModel['periodo'] == '2019-12-01'].copy()

# ------------------------------------------------------------------
# 2) PREPROCESAMIENTO SIN ‘LEAKAGE’
# ------------------------------------------------------------------
# ------------------------------------------------------------------
# 2) PREPROCESAMIENTO SIN ‘LEAKAGE’
# ------------------------------------------------------------------
def prep_strict(df: pd.DataFrame) -> pd.DataFrame:
    """
    Elimina columnas de fuga + cualquier datetime + tipa categóricas.
    """
    # 1) Quitamos las columnas indicadas
    X = df.drop(columns=DROP_COLS_NOW, errors="ignore").copy()

    # 2) Además, eliminamos cualquier columna datetime que quedase
    dt_cols = X.select_dtypes(include=['datetime64[ns]']).columns.tolist()
    if dt_cols:
        X.drop(columns=dt_cols, inplace=True)

    # 3) Tipeamos las categóricas
    for c in cat_feats:
        if c in X.columns:
            X[c] = X[c].astype('category')
    return X

USE_LOG_TARGET = True                 # ← pon False si prefieres toneladas
TARGET_COL     = 'tn_log' if USE_LOG_TARGET else 'tn'

X_tr   = prep_strict(train_oct)
y_tr   = train_oct[TARGET_COL]

X_val  = prep_strict(valid_nov)
y_val  = valid_nov[TARGET_COL]

X_test = prep_strict(test_dec)
y_true_tn = test_dec['tn']            # tn reales para métricas finales


# ------------------------------------------------------------------
# 4) MODELOS
# ------------------------------------------------------------------
seed = 42
params_base = {
    "objective": "regression",
    "metric": "mae",
    "learning_rate": 0.1,
    "num_leaves": 50,
    "seed": seed,
    "feature_fraction_seed": seed,
    "bagging_seed": seed,
    "nthread": 1,
    "deterministic": True
}

params_tuned = {
    "objective": "regression",
    "metric": "mae",
    "learning_rate": 0.08813259224734658,
    "num_leaves": 79,
    "min_child_samples": 100,
    "feature_fraction": 0.9833497436436693,
    "bagging_fraction": 0.9951622648438583,
    "bagging_freq": 0,
    "lambda_l2": 0.3939212978268534,
    "seed": 42,
    "feature_fraction_seed": 42,
    "bagging_seed": 42,
    "nthread": 1,
    "deterministic": True,
    "verbose": -1
}


base_model = train_lgb(X_tr, y_tr, X_val, y_val,
                       params_base, num_rounds=1000, early_stop=50, name='BASE')

tuned_model = train_lgb(X_tr, y_tr, X_val, y_val,
                        params_tuned, num_rounds=4000, early_stop=200, name='TUNED')

def wmape(y_true, y_pred, eps=1e-8):
    return np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + eps)


# ------------------------------------------------------------------
# 5) EVALUACIÓN EN DIC-19  (toneladas reales)
# ------------------------------------------------------------------
for tag, model in [("BASE", base_model), ("TUNED", tuned_model)]:
    y_pred_log = model.predict(X_test, num_iteration=model.best_iteration)
    y_pred_tn  = np.expm1(y_pred_log) if USE_LOG_TARGET else y_pred_log

    mae   = mean_absolute_error(y_true_tn, y_pred_tn)
    mape  = mape_safe(y_true_tn, y_pred_tn)
    wmAPE = wmape(y_true_tn, y_pred_tn)

    print(f"{tag:>5} | MAE: {mae:.2f} | MAPE: {mape:.2%} | WMAPE: {wmAPE:.2%}")

"""modelo con tweedie

LightGbm Tweedie busqueda de Powers
"""

# ------------------------------------------------------------------
# Loop rápido de p Tweedie y captura de resultados
# ------------------------------------------------------------------
powers = [1.1, 1.2, 1.3, 1.4, 1.5]
results = []

for p in powers:
    params_p = params_base.copy()
    params_p.update({
        "objective": "tweedie",
        "tweedie_variance_power": p,
    })
    model_p = train_lgb(
        X_tr, y_tr,
        X_val, y_val,
        params_p,
        num_rounds=4000,
        early_stop=200,
        name=f"TWEED_p{p}"
    )

    # --- eval en dic-19 ---
    y_pred = np.expm1(model_p.predict(X_test, num_iteration=model_p.best_iteration))
    wm    = wmape(y_true_tn, y_pred)
    results.append((p, wm))
    print(f"p={p:.1f} | WMAPE: {wm:.2%}")

# ------------------------------------------------------------------
# Extraer el mejor p
# ------------------------------------------------------------------
best_p, best_wmape = min(results, key=lambda x: x[1])
print(f"\nMejor p = {best_p:.1f} con WMAPE = {best_wmape:.2%}")

# ------------------------------------------------------------------
# 1) Definimos hiper-parámetros usando best_p
# ------------------------------------------------------------------
params_tweed_opt = params_base.copy()
params_tweed_opt.update({
    "objective":              "tweedie",
    "tweedie_variance_power": best_p,
    # Opcionalmente afina learning_rate, num_leaves, etc.
})

# ------------------------------------------------------------------
# 2) Entrenamos el modelo final con el best_p
# ------------------------------------------------------------------
tweed_opt_model = train_lgb(
    X_tr, y_tr,
    X_val, y_val,
    params_tweed_opt,
    num_rounds=4000,
    early_stop=300,
    name=f"TWEED_p{best_p:.1f}"
)

# ------------------------------------------------------------------
# 3) Evaluación comparativa en DIC-19
# ------------------------------------------------------------------
for tag, model in [("BASE",    base_model),
                   ("TUNED",   tuned_model),
                   (f"TWEED_{best_p:.1f}", tweed_opt_model)]:
    y_pred_log = model.predict(X_test, num_iteration=model.best_iteration)
    y_pred_tn  = np.expm1(y_pred_log) if USE_LOG_TARGET else y_pred_log

    mae   = mean_absolute_error(y_true_tn, y_pred_tn)
    mape  = mape_safe(y_true_tn, y_pred_tn)
    wmAPE = wmape(y_true_tn, y_pred_tn)

    print(f"{tag:>10} | MAE: {mae:.2f} | MAPE: {mape:.2%} | WMAPE: {wmAPE:.2%}")

"""comparativo

###busqueda de hiper parametros con optuna tomar la segunda celda la primera es dejaral

este optuna ya tiene los hiperparametros hardcodeados , despues de 2hs de correr otro que encontr lo s mejores
"""

# Redefinimos WMAPE con nombre nuevo para evitar colisiones
def compute_wmape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:
    return np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + eps)

#este codigo funciona bien


import optuna
import copy
import numpy as np
import pandas as pd
import lightgbm as lgb  # Import lightgbm
from sklearn.metrics import mean_absolute_error

# -------------------------------------------------------------------
# Precondiciones: ya tienes X_tr, y_tr (log1p), X_val, y_val,
#                wmape(y_true, y_pred) y train_lgb(...) definidos,
#                params_base con tu base Tweedie
# -------------------------------------------------------------------

# ╔══════════════════════════════════════════════════════════╗
#   Optuna objective  ▶  búsqueda fina alrededor del best set
# ╚══════════════════════════════════════════════════════════╝
def objective(trial):
    """
    Parte del best-set que encontraste y ajusta +/- un rango razonable.
    Devuelve WMAPE (más bajo = mejor).
    """
    params = params_base.copy() | {
        "objective": "regression",
        "metric":    "mae",
        "seed":      42,
        "deterministic": True,
        # ---------- hp que SÍ ajustaremos ----------
        "tweedie_variance_power": trial.suggest_float("p",
                                   1.05, 1.25, step=0.01),     # alrededor 1.14
        "learning_rate":          trial.suggest_float("lr",
                                   0.015, 0.05, log=True),     # alrededor 0.03
        "num_leaves":             trial.suggest_int("num_leaves",
                                   16, 48, step=4),            # alrededor 23
        "min_child_samples":      trial.suggest_int("min_child_samples",
                                   120, 250, step=10),         # alrededor 188
        "feature_fraction":       trial.suggest_float("feature_fraction",
                                   0.55, 0.85),                # alrededor 0.63
        "bagging_fraction":       trial.suggest_float("bagging_fraction",
                                   0.55, 0.85),                # alrededor 0.66
        "lambda_l2":              trial.suggest_float("lambda_l2",
                                   0.0, 1.0),                  # dejamos libre
    }

    # regla para bagging_freq
    params["bagging_freq"] = 1 if params["bagging_fraction"] < 1.0 else 0

    # ---------- entrenamiento ----------
    booster = train_lgb(
        X_tr, y_tr,
        X_val, y_val,
        params,
        num_rounds=2000,          # menos vueltas: exploración más rápida
        early_stop=200,
        name=f"opt-{trial.number}"
    )

    trial.set_user_attr("best_iteration", booster.best_iteration)

    # ---------- métrica a minimizar ----------
    y_pred = np.expm1(booster.predict(X_val, num_iteration=booster.best_iteration))
    y_true = np.expm1(y_val)
    return compute_wmape(y_true, y_pred)

# ────────── Lanza la búsqueda (40 trials) ──────────
study = optuna.create_study(direction="minimize",
                            sampler=optuna.samplers.TPESampler(seed=42))
study.optimize(objective, n_trials=65, show_progress_bar=True)

print("■ Mejor WMAPE :", study.best_value)
print("■ Mejores hps :", study.best_params)
print("■ best_iter   :", study.best_trial.user_attrs['best_iteration'])

# -------------------------------------------------------------------
#  5) Re-entrena el modelo final con train+valid (sin mismatch cats)
# -------------------------------------------------------------------

# 5.1) Construye X_trval, y_trval, w_trval
X_trval = pd.concat([X_tr, X_val])
y_trval = pd.concat([y_tr, y_val])
w_trval = np.expm1(y_trval)

# 5.2) Función para codificar categorías
def encode_cats(df, cat_feats):
    df2 = df.copy()
    for c in cat_feats:
        if c in df2.columns:
            df2[c] = pd.Categorical(df2[c]).codes.astype("int32")
    return df2

# 5.3) Numéricos para train y valid
X_trval_num = encode_cats(X_trval, cat_feats)
X_val_num   = encode_cats(X_val,   cat_feats)

# 5.4) Crea los Datasets sin pasar categorical_feature
dtrain_full = lgb.Dataset(
    X_trval_num, y_trval,
    weight=w_trval
)
dvalid = lgb.Dataset(
    X_val_num, y_val,
    weight=np.expm1(y_val)
)

# 5.5) Monta best_params y best_iteration para el re-entrenamiento
best_hyperparams = study.best_params
best_iteration   = study.best_trial.user_attrs["best_iteration"]

# Combina con tu params_base original
best_params = params_base.copy()
best_params.update(best_hyperparams)

# 5.6) Ahora entrena usando early-stopping y tu feval de WMAPE
final_model = lgb.train(
    best_params,
    dtrain_full,
    num_boost_round=best_iteration,       # usa la iteración óptima
    valid_sets=[dtrain_full, dvalid],
    valid_names=["train_full", "valid"],
    callbacks=[
        lgb.log_evaluation(period=200),
        lgb.early_stopping(stopping_rounds=200, verbose=False)
    ],
    feval=lgb_wmape_eval
)

print("✅ final_model re-entrenado sin mismatch de cats")

from pprint import pprint   # sólo para un print bonito

# --- hiperparámetros del booster ---
pprint(final_model.params)

"""No correr si no se considera necesario - Optuna 250 iteraciones - no cambia mucho no vale la pena

###Comparativo de modelos y alternativas lightGbm

Modelos a comparar
"""

# ------------------------------------------------------------------
# 3)  Evaluación - aca hay que agruegar el modelo neuvo - ya estan los uqe ya entrenamos
# ------------------------------------------------------------------
model_list = [
    ("BASE",   base_model),
    ("TUNED",  tuned_model),
    ("TWEED",  tweed_opt_model),
    ("FINAL",  final_model)
]

from sklearn.metrics import mean_absolute_error
import numpy as np, pandas as pd

# ------------------------------------------------------------------
# 1)  Capturamos el orden de categorías del entrenamiento
# ------------------------------------------------------------------
cat_levels = {
    c: pd.Categorical(X_trval[c]).categories
    for c in cat_feats
}

def encode_like_train(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for c, cats in cat_levels.items():
        if c in out.columns:
            out[c] = (
                pd.Categorical(out[c], categories=cats)
                  .codes.astype("int32")
            )
    return out

# ------------------------------------------------------------------
# 2)  Función WMAPE segura (no será sobrescrita)
# ------------------------------------------------------------------
def wmape_func(y_true, y_pred, eps=1e-8):
    return np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + eps)

# ------------------------------------------------------------------
# 3)  Evaluación
# ------------------------------------------------------------------
model_list = [
    ("BASE",   base_model),
    ("TUNED",  tuned_model),
    ("TWEED",  tweed_opt_model),
    ("FINAL",  final_model)
]

for tag, model in model_list:
    cols   = model.feature_name()
    X_eval = encode_like_train(X_test[cols])
    X_mat  = X_eval.to_numpy(dtype=np.float32)

    y_pred = np.expm1(
        model.predict(X_mat, num_iteration=model.best_iteration or 1)
    )
    mae   = mean_absolute_error(y_true_tn, y_pred)
    mape  = mape_safe(y_true_tn, y_pred)
    wmp   = wmape_func(y_true_tn, y_pred)

    print(f"{tag:>6} | MAE: {mae:.2f} | MAPE: {mape:.2%} | WMAPE: {wmp:.2%}")

"""si queremos agrergar modelos hay que hacerlo aca -- y luego lo nombramos choosen Model en la seccion siguiente

### Modelo nuevo - Agregar aqui

**Agregamos el modelo nuevo a la lista de abajo, por favor tener en cuenta que si copiamos una celda de arriba hay que cambiar el nombre del output si no sobreescribimos lo que ya hicimos --**

vamos a probar los mismos modelos sin log
"""

# ------------------------------------------------------------------
# CONFIGURACIÓN
# ------------------------------------------------------------------
import pandas as pd, numpy as np, lightgbm as lgb
from sklearn.metrics import mean_absolute_error

# ------------------------------------------------------------------
# CONFIGURACIÓN
# ------------------------------------------------------------------

DROP_COLS_NOW = [
    'tn',       # target
    'tn_log',   # model target
    'periodo',  # <— must drop the datetime
    'cust_request_tn'
    #'ratio_tn_stock',
    #'flag_stock_disp',

    #'stock_t0'
]
cat_feats = ['cat1','cat2','cat3','brand','month','quarter']


# ------------------------------------------------------------------
# 1) SPLITS
# ------------------------------------------------------------------
train_oct = readyforModel[readyforModel['periodo'] <= '2019-10-01'].copy()
valid_nov = readyforModel[readyforModel['periodo'] == '2019-11-01'].copy()
test_dec  = readyforModel[readyforModel['periodo'] == '2019-12-01'].copy()

# ------------------------------------------------------------------
# 2) PREPROCESAMIENTO SIN ‘LEAKAGE’
# ------------------------------------------------------------------
# ------------------------------------------------------------------
# 2) PREPROCESAMIENTO SIN ‘LEAKAGE’
# ------------------------------------------------------------------
def prep_strict(df: pd.DataFrame) -> pd.DataFrame:
    """
    Elimina columnas de fuga + cualquier datetime + tipa categóricas.
    """
    # 1) Quitamos las columnas indicadas
    X = df.drop(columns=DROP_COLS_NOW, errors="ignore").copy()

    # 2) Además, eliminamos cualquier columna datetime que quedase
    dt_cols = X.select_dtypes(include=['datetime64[ns]']).columns.tolist()
    if dt_cols:
        X.drop(columns=dt_cols, inplace=True)

    # 3) Tipeamos las categóricas
    for c in cat_feats:
        if c in X.columns:
            X[c] = X[c].astype('category')
    return X

USE_LOG_TARGET = False                 # ← pon False si prefieres toneladas
TARGET_COL     = 'tn_log' if USE_LOG_TARGET else 'tn'

X_tr   = prep_strict(train_oct)
y_tr   = train_oct[TARGET_COL]

X_val  = prep_strict(valid_nov)
y_val  = valid_nov[TARGET_COL]

X_test = prep_strict(test_dec)
y_true_tn = test_dec['tn']            # tn reales para métricas finales


# ------------------------------------------------------------------
# 4) MODELOS
# ------------------------------------------------------------------
seed = 42
params_base = {
    "objective": "regression",
    "metric": "mae",
    "learning_rate": 0.1,
    "num_leaves": 50,
    "seed": seed,
    "feature_fraction_seed": seed,
    "bagging_seed": seed,
    "nthread": 1,
    "deterministic": True
}

params_tuned = {
    "objective": "regression",
    "metric": "mae",
    "learning_rate": 0.08813259224734658,
    "num_leaves": 79,
    "min_child_samples": 100,
    "feature_fraction": 0.9833497436436693,
    "bagging_fraction": 0.9951622648438583,
    "bagging_freq": 0,
    "lambda_l2": 0.3939212978268534,
    "seed": 42,
    "feature_fraction_seed": 42,
    "bagging_seed": 42,
    "nthread": 1,
    "deterministic": True,
    "verbose": -1
}


base_model_sinLog = train_lgb(X_tr, y_tr, X_val, y_val,
                       params_base, num_rounds=1000, early_stop=50, name='BASE')

tuned_model_sinLog = train_lgb(X_tr, y_tr, X_val, y_val,
                        params_tuned, num_rounds=4000, early_stop=200, name='TUNED')

def wmape(y_true, y_pred, eps=1e-8):
    return np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + eps)


# ------------------------------------------------------------------
# 5) EVALUACIÓN EN DIC-19  (toneladas reales)
# ------------------------------------------------------------------
for tag, model in [("BASE", base_model), ("TUNED", tuned_model)]:
    y_pred_log = model.predict(X_test, num_iteration=model.best_iteration)
    y_pred_tn  = np.expm1(y_pred_log) if USE_LOG_TARGET else y_pred_log

    mae   = mean_absolute_error(y_true_tn, y_pred_tn)
    mape  = mape_safe(y_true_tn, y_pred_tn)
    wmAPE = wmape(y_true_tn, y_pred_tn)

    print(f"{tag:>5} | MAE: {mae:.2f} | MAPE: {mape:.2%} | WMAPE: {wmAPE:.2%}")

# ------------------------------------------------------------------
# Loop rápido de p Tweedie y captura de resultados
# ------------------------------------------------------------------
powers = [1.1, 1.2, 1.3, 1.4, 1.5]
results = []

for p in powers:
    params_p = params_base.copy()
    params_p.update({
        "objective": "tweedie",
        "tweedie_variance_power": p,
    })
    model_p1 = train_lgb(
        X_tr, y_tr,
        X_val, y_val,
        params_p,
        num_rounds=4000,
        early_stop=200,
        name=f"TWEED_p{p}"
    )

    # --- eval en dic-19 ---
    y_pred = model_p1.predict(X_test, num_iteration=model_p.best_iteration)
    wm    = wmape(y_true_tn, y_pred)
    results.append((p, wm))
    print(f"p={p:.1f} | WMAPE: {wm:.2%}")

# ------------------------------------------------------------------
# Extraer el mejor p
# ------------------------------------------------------------------
best_p, best_wmape = min(results, key=lambda x: x[1])
print(f"\nMejor p = {best_p:.1f} con WMAPE = {best_wmape:.2%}")

# ------------------------------------------------------------------
# 1) Definimos hiper-parámetros usando best_p
# ------------------------------------------------------------------
params_tweed_opt = params_base.copy()
params_tweed_opt.update({
    "objective":              "tweedie",
    "tweedie_variance_power": best_p,
    # Opcionalmente afina learning_rate, num_leaves, etc.
})

# ------------------------------------------------------------------
# 2) Entrenamos el modelo final con el best_p
# ------------------------------------------------------------------
tweed_opt_model_sinLog = train_lgb(
    X_tr, y_tr,
    X_val, y_val,
    params_tweed_opt,
    num_rounds=4000,
    early_stop=300,
    name=f"TWEED_p{best_p:.1f}"
)

# ------------------------------------------------------------------
# 3) Evaluación comparativa en DIC-19
# ------------------------------------------------------------------
for tag, model in [("BASE SIN LOG",    base_model_sinLog),
                   ("TUNED SIN LOG",   tuned_model_sinLog),
                   (f"TWEED_{best_p:.1f} SIN LOG ", tweed_opt_model_sinLog)]:
    y_pred_log = model.predict(X_test, num_iteration=model.best_iteration)
    y_pred_tn  = np.expm1(y_pred_log) if USE_LOG_TARGET else y_pred_log

    mae   = mean_absolute_error(y_true_tn, y_pred_tn)
    mape  = mape_safe(y_true_tn, y_pred_tn)
    wmAPE = wmape(y_true_tn, y_pred_tn)

    print(f"{tag:>10} | MAE: {mae:.2f} | MAPE: {mape:.2%} | WMAPE: {wmAPE:.2%}")

# ------------------------------------------------------------------
# 3)  Evaluación - aca hay que agruegar el modelo neuvo - ya estan los uqe ya entrenamos
# ------------------------------------------------------------------
model_list = [
    ("BASE",   base_model),
    ("TUNED",  tuned_model),
    ("TWEED",  tweed_opt_model),
    ("FINAL",  final_model),
    ("Base Sin log",  base_model_sinLog),
    ("TUNED sin log",  tuned_model_sinLog),
    ("TWEED sin log ",  tweed_opt_model_sinLog)
]

# ------------------------------------------------------------------
# 1)  Capturamos el orden de categorías del entrenamiento
# ------------------------------------------------------------------
cat_levels = {
    c: pd.Categorical(X_trval[c]).categories
    for c in cat_feats
}

def encode_like_train(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for c, cats in cat_levels.items():
        if c in out.columns:
            out[c] = (
                pd.Categorical(out[c], categories=cats)
                  .codes.astype("int32")
            )
    return out

# ------------------------------------------------------------------
# 2)  Función WMAPE segura (no será sobrescrita)
# ------------------------------------------------------------------
def wmape_func(y_true, y_pred, eps=1e-8):
    return np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + eps)

# ---------------------------------------------------------------
# 0) IMPORTS
# ---------------------------------------------------------------
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error

# ---------------------------------------------------------------
# 1)  Captura los niveles de *todas* las columnas categóricas
# ---------------------------------------------------------------
cat_levels = {}
for c in X_trval.select_dtypes(include='category').columns:
    cat_levels[c] = pd.Categorical(X_trval[c]).categories

def encode_like_train(df: pd.DataFrame) -> pd.DataFrame:
    """Convierte cualquier columna 'category' al código entero
       coherente con el entrenamiento. Crea columnas faltantes = -1."""
    out = df.copy()

    # Convierte cada col categórica
    for c, cats in cat_levels.items():
        if c not in out.columns:
            out[c] = -1                       # si faltaba, todo NA
        out[c] = (
            pd.Categorical(out[c], categories=cats)
              .codes
              .astype("int32")
        )

    # Asegúrate de que no queden columnas 'category'
    cat_cols_left = out.select_dtypes(include='category').columns
    if len(cat_cols_left):
        out[cat_cols_left] = out[cat_cols_left].apply(
            lambda s: s.cat.codes.astype("int32")
        )

    return out

# ---------------------------------------------------------------
# 2)  Métricas auxiliares (por si no las tenías)
# ---------------------------------------------------------------
def mape_safe(y_true, y_pred, eps=1e-9):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    return np.mean(np.abs((y_true - y_pred) /
                          np.maximum(np.abs(y_true), eps)))

def wmape_func(y_true, y_pred):
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

# ---------------------------------------------------------------
# 3)  Constantes clipping (para modelos entrenados en log)
# ---------------------------------------------------------------
MAX_LOG, MIN_LOG = 20.0, -20.0           # ajusta si hiciera falta

def to_tn(log_pred, needs_expm1):
    if needs_expm1:
        log_pred = np.clip(log_pred, MIN_LOG, MAX_LOG)
        return np.expm1(log_pred)
    return log_pred

# ---------------------------------------------------------------
# 4)  Pre‑codifica X_test una vez
# ---------------------------------------------------------------
X_test_enc = encode_like_train(X_test)

# ---------------------------------------------------------------
# 5)  Evaluación comparativa
# ---------------------------------------------------------------
print("🔍  Evaluación (MAE / MAPE / WMAPE):\n")

for tag, model in model_list:
    cols = model.feature_name()                 # orden original
    X_mat = X_test_enc[cols].to_numpy(dtype=np.float32)

    log_pred = model.predict(X_mat,
                             num_iteration=model.best_iteration or 1)

    needs_expm1 = 'sin log' not in tag.lower()  # aplica sólo a modelos log
    y_pred = to_tn(log_pred, needs_expm1)

    if not np.all(np.isfinite(y_pred)):
        raise ValueError(f"{tag}: NaN o Inf detectados en y_pred")

    mae  = mean_absolute_error(y_true_tn, y_pred)
    mape = mape_safe(y_true_tn, y_pred)
    wmp  = wmape_func(y_true_tn, y_pred)

    print(f"{tag:>16} | MAE: {mae:,.2f} | "
          f"MAPE: {mape:.2%} | WMAPE: {wmp:.2%}")

"""Agregamos el modelo nuevo a la lista de modelos a comparar

Comparamos los modelos y elegimos cual va a ser el choosen model

### Resutlados para el mejor modelo - Lo levantamos del output anterior
"""

model_list = [
    ("BASE",   base_model),
    ("TUNED",  tuned_model),
    ("TWEED",  tweed_opt_model),
    ("FINAL",  final_model),
    ("Base Sin log",  base_model_sinLog),
    ("TUNED sin log",  tuned_model_sinLog),
    ("TWEED sin log ",  tweed_opt_model_sinLog)
]

"""# Elemnto clave del pipeline nos permite cambiar a log sin log y usar el mismo codigo"""

Choosen_model=tuned_model  #agarramos el modelo que queremos y lo igualamos a c
sinlog        = False                   # ← True  si el target NO está en log
                                         # ← False si el modelo devuelve log

# ────────────────────────────────────────────────────────────────────
# 0) Elige modelo y marca si viene SIN LOG
# ────────────────────────────────────────────────────────────────────
Choosen_model = tuned_model   # ← cambia según el que pruebes
sinlog        = False                # ← True  si el target NO está en log
                                    # ← False si el modelo devuelve log‑tn

# ────────────────────────────────────────────────────────────────────
# 1) Prepara X_test alineado al modelo
# ────────────────────────────────────────────────────────────────────
feat_used = Choosen_model.feature_name()
X_eval = X_test[feat_used].copy()

for c in cat_feats:
    if c in X_eval.columns:
        # convierte categorías a códigos numéricos homogéneos
        X_eval[c] = pd.Categorical(X_eval[c]).codes.astype("float32")

X_mat = X_eval.to_numpy(dtype=np.float32)

# ────────────────────────────────────────────────────────────────────
# 2) Predice y convierte a tn reales si procede
# ────────────────────────────────────────────────────────────────────
raw_pred = Choosen_model.predict(X_mat,
                                 num_iteration=Choosen_model.best_iteration or 1)

if sinlog:
    y_pred_tn = raw_pred                     # ya está en tn
else:
    # clipping anti‑overflow (opcional, ajusta límites si lo necesitas)
    raw_pred = np.clip(raw_pred, -20, 20)    # exp(20)-1 ≈ 4.8e8
    y_pred_tn = np.expm1(raw_pred)           # log → tn

y_true = y_true_tn                           # ya definido como test_dec['tn']

# ────────────────────────────────────────────────────────────────────
# 3) Métricas
# ────────────────────────────────────────────────────────────────────
mae   = mean_absolute_error(y_true, y_pred_tn)
wmape = np.sum(np.abs(y_true - y_pred_tn)) / np.sum(np.abs(y_true))

print(f"► Diciembre‑19 | MAE: {mae:.3f} tn | WMAPE: {wmape:.2%}")

# ────────────────────────────────────────────────────────────────────
# 4) DataFrame de predicciones
# ────────────────────────────────────────────────────────────────────
df_preds = pd.DataFrame({
    "product_id": test_dec["product_id"].values,
    "real_tn":    y_true,
    "pred_tn":    y_pred_tn
})
df_preds["abs_error"] = np.abs(df_preds["real_tn"] - df_preds["pred_tn"])

# ────────────────────────────────────────────────────────────────────
# 5) Mostrar primeras 10 filas
# ────────────────────────────────────────────────────────────────────
print(df_preds.head(10))

"""sacamos los features asi despues ya los podemos usar"""

feat_used = Choosen_model.feature_name()   # ← lista de 62 columnas reales

print(feat_used)

"""***Prediccion enero 2020***

Creamos dataset enero
"""

ENE = "2020-01-01"


# ------------------------------------------------------------------
# 4) Crear prototipo enero-20 vacío y concatenar
# ------------------------------------------------------------------
base_cols = df_final.columns.difference(["tn", "cust_request_tn"])
jan_proto = (
    df_final[df_final["periodo"] == "2019-12-01"][base_cols]
      .assign(periodo=pd.to_datetime(ENE),
              tn=np.nan,
              cust_request_tn=np.nan)
)
df_ene = pd.concat([df_final, jan_proto], ignore_index=True)
df_ene["periodo"] = pd.to_datetime(df_ene["periodo"])
# 2) Añade un mes de antigüedad extra
df_ene["antiguedad_meses"] = df_ene["antiguedad_meses"] + 1

"""Mergeamos con stock

"""

# ------------------------------------------------------------------
# 1) MERGE con stock desplazado +1 mes (stock_t0)
# ------------------------------------------------------------------
stock = (
    df_tb_stocks_02.copy()
        .assign(periodo=lambda d:
                pd.to_datetime(d["periodo"].astype(str), format="%Y%m")
                + pd.offsets.MonthEnd(0)      # 201810 → 2018-10-31
                + pd.offsets.MonthBegin(1))   #        → 2018-11-01
        .rename(columns={"stock_final": "stock_t0"})
)

df_enefeatsandStock = df_ene.merge(
    stock[["product_id", "periodo", "stock_t0"]],
    on=["product_id", "periodo"],
    how="left"
)

"""Este dataset es el dataset que vamos a usar para predecir enero , con el modelo entrenado para diciembre - el choosen modelo - es decir el mejor modelo en funcion de la comparativa anterior"""

df_enefeatsandStock.tail()

"""ahora le aplicamos el Feature engineering

aplicamos el feature engineering
"""

df_eneTuned = feature_pipeline(df_enefeatsandStock)

df_eneTuned

"""hacemos cuadrar los features a la fuerza"""

feat_used_count = len(feat_used)
X_ene_columns_count = len(df_eneTuned.columns)

print(f"Count of columns in feat_used: {feat_used_count}")
print(f"Count of columns in X_ene.columns: {X_ene_columns_count}")

if feat_used_count == X_ene_columns_count:
    print("The number of columns matches!")
else:
    print("The number of columns DOES NOT match.")

def encode_cats(df, cat_feats):
    df2 = df.copy()
    for c in cat_feats:
        if c in df2.columns:
            # pd.Categorical works on any dtype, then .codes → int
            df2[c] = pd.Categorical(df2[c]).codes.astype("int32")
    return df2

# 5.3) Numéricos para train y valid
df_eneTuned = encode_cats(df_eneTuned, cat_feats)

# -------------------------------------------------

df_eneTuned.head()

"""Predecimos enero con nuestro mejor modelo"""

import numpy as np
import pandas as pd

# ────────────────────────────────────────────────────────────────────
# Listas ya definidas
# ────────────────────────────────────────────────────────────────────
DROP_COLS_NOW = [
    'tn',        # target
    'tn_log',    # model target
    'periodo',   # será usado para filtrar antes de borrar
    'cust_request_tn'
]
cat_feats = ['cat1','cat2','cat3','brand','month','quarter']

# ────────────────────────────────────────────────────────────────────
# 1) Codifica todas las categóricas en df_eneTuned
# ────────────────────────────────────────────────────────────────────
def encode_cats(df, cat_feats):
    df2 = df.copy()
    for c in cat_feats:
        if c in df2.columns:
            df2[c] = pd.Categorical(df2[c]).codes.astype("int32")
    return df2

df_eneTuned = encode_cats(df_eneTuned, cat_feats)

# ────────────────────────────────────────────────────────────────────
# 2) Filtra solo enero usando 'periodo'
# ────────────────────────────────────────────────────────────────────
mask_jan = df_eneTuned["periodo"] == ENE
df_jan   = df_eneTuned.loc[mask_jan].copy()

# ────────────────────────────────────────────────────────────────────
# 3) Ahora sí, elimina las columnas que no entran al modelo
# ────────────────────────────────────────────────────────────────────
df_jan_model = df_jan.drop(columns=DROP_COLS_NOW, errors='ignore')

# ────────────────────────────────────────────────────────────────────
# 4) Alinea columnas con las que vio el modelo
# ────────────────────────────────────────────────────────────────────
feat_used = Choosen_model.feature_name()
X_jan     = df_jan_model[feat_used].copy()  # KeyError si falta alguna

# ────────────────────────────────────────────────────────────────────
# 5) Prepara matriz y predice
# ────────────────────────────────────────────────────────────────────
# 5) Prepara matriz y predice
X_mat     = X_jan.to_numpy(dtype=np.float32)
best_it   = getattr(Choosen_model, "best_iteration", 1)
y_predlog = Choosen_model.predict(X_mat, num_iteration=best_it)

# --- APLICAR O NO expm1 SEGÚN EL FLAG ---------------------------------
if sinlog:                                   # ← modelo SIN log
    y_pred_tn = y_predlog                    # ya está en tn reales
    y_predlog = np.log1p(y_predlog)          # (opcional) guardas el log
else:                                        # ← modelo CON log
    y_pred_tn = np.expm1(np.clip(y_predlog, -20, 20))  # log → tn

# ────────────────────────────────────────────────────────────────────
# 6) Añade las predicciones a df_jan
# ────────────────────────────────────────────────────────────────────
df_jan["y_pred_log"] = y_predlog
df_jan["pred_tn"]    = np.expm1(y_predlog)

# ────────────────────────────────────────────────────────────────────
# 7) (Opcional) Inserta de vuelta en el DataFrame completo
# ────────────────────────────────────────────────────────────────────
df_eneTuned.loc[mask_jan, ["y_pred_log","pred_tn"]] = df_jan[["y_pred_log","pred_tn"]]

# ────────────────────────────────────────────────────────────────────
# 8) Vista rápida
# ────────────────────────────────────────────────────────────────────
print(df_jan[["periodo","product_id","y_pred_log","pred_tn"]].head(10))

"""chequeo"""

ids = [20001, 20002, 20004, 20008]

# Opción habitual
df_filtrado = df_ene[df_ene["product_id"].isin(ids)]
df_filtrado.tail(50)

"""Asignamos Tn la prediccion"""

# 6b) Asigna la predicción al campo 'tn' original
df_jan["tn"] = df_jan["pred_tn"]

# Si prefieres hacerlo sobre df_eneTuned y no en la copia df_jan:
df_eneTuned.loc[mask_jan, "tn"] = df_jan["pred_tn"].values

df_jan.head()

"""Agregamos TN el dataset del princpio que usamos para estimar enero antes de pasarlo por el pipeline"""

df_enefeatsandStock

# Suponemos que df_jan ya tiene ['product_id','pred_tn'] para enero
pred_jan = df_jan[['product_id', 'pred_tn']].rename(columns={'pred_tn': 'tn_pred'})

# Hacemos un left‐join sobre product_id (y opcionalmente periodo si quieres)
df_merged = (
    df_enefeatsandStock
    .merge(pred_jan, on='product_id', how='left')
)

# Sobreescribimos tn con la predicción donde exista
df_merged['tn'] = df_merged['tn_pred'].fillna(df_merged['tn'])

# (Opcional) Eliminamos la columna auxiliar
df_merged = df_merged.drop(columns=['tn_pred'])

# Resultado
print(df_merged[['periodo','product_id','tn']].head())

df_merged.tail()

# Guarda el DataFrame en CSV
df_merged.to_csv('/content/sample_data/df_predenero_optuna.csv', index=False)
print("✅ CSV guardado en /content/sample_data/df_predenero_optuna.csv")

"""**Prediccion Febrero**

A partir del dataset mergeado - Enero mas predicciones de enero - repetimos el proceso para febrero
"""

FEB = "2020-02-01"


# ------------------------------------------------------------------
# 4) Crear prototipo febero-20 vacío y concatenar
# ------------------------------------------------------------------
base_cols = df_merged.columns.difference(["tn", "cust_request_tn"])
feb_proto = (
    df_merged[df_merged["periodo"] == "2020-01-01"][base_cols]
      .assign(periodo=pd.to_datetime(FEB),
              tn=np.nan,
              cust_request_tn=np.nan)
)
df_feb= pd.concat([df_merged, feb_proto], ignore_index=True)
df_feb["periodo"] = pd.to_datetime(df_feb["periodo"])
# 2) Añade un mes de antigüedad extra
df_feb["antiguedad_meses"] = df_feb["antiguedad_meses"] + 1

"""como necesitamos stcok le ponemos el promedio historico para el mes de febrero"""

# Constante de febrero
FEB = pd.to_datetime("2020-02-01")

# 1) Calcula el stock_t0 promedio histórico por producto para mes=2
mask_feb_hist = df_merged["periodo"].dt.month == 2
avg_stock = (
    df_merged.loc[mask_feb_hist]
             .groupby("product_id")["stock_t0"]
             .mean()
             .rename("stock_t0_avg")
)

# 2) Asigna ese promedio a las filas de febrero en df_feb
is_feb = df_feb["periodo"] == FEB

# Usa map para mantener el índice
df_feb.loc[is_feb, "stock_t0"] = (
    df_feb.loc[is_feb, "product_id"]
          .map(avg_stock)
)

# 3) Comprueba
print("Febrero filas:", is_feb.sum())
print(df_feb.loc[is_feb, ["product_id","stock_t0"]].head())

df_feb.tail(25)

"""aplicamos feature engineering a deuvelta - lo tenemos que hacer devuelta sobre todo el dataframe si no las los deltas y los lags se rompen , es por eso que todo el fine tunning se  hace nuevamente"""

df_febTuned = feature_pipeline(df_feb)

df_febTuned.columns

import numpy as np
import pandas as pd

# ────────────────────────────────────────────────────────────────────
# Listas ya definidas (idénticas a las de enero)
# ────────────────────────────────────────────────────────────────────
DROP_COLS_NOW = [
    'tn',        # target
    'tn_log',    # model target
    'periodo',   # será usado para filtrar antes de borrar
    'cust_request_tn'
]
cat_feats = ['cat1','cat2','cat3','brand','month','quarter']

# Constante de febrero
FEB = pd.to_datetime("2020-02-01")

# ────────────────────────────────────────────────────────────────────
# 1) Codifica todas las categóricas en df_febTuned
# ────────────────────────────────────────────────────────────────────
def encode_cats(df, cat_feats):
    df2 = df.copy()
    for c in cat_feats:
        if c in df2.columns:
            df2[c] = pd.Categorical(df2[c]).codes.astype("int32")
    return df2

df_febTuned = encode_cats(df_febTuned, cat_feats)

# ────────────────────────────────────────────────────────────────────
# 2) Filtra solo febrero usando 'periodo'
# ────────────────────────────────────────────────────────────────────
mask_feb = df_febTuned["periodo"] == FEB
df_feb   = df_febTuned.loc[mask_feb].copy()

# ────────────────────────────────────────────────────────────────────
# 3) Elimina las columnas que no entran al modelo
# ────────────────────────────────────────────────────────────────────
df_feb_model = df_feb.drop(columns=DROP_COLS_NOW, errors='ignore')

# ────────────────────────────────────────────────────────────────────
# 4) Alinea columnas con las que vio el modelo
# ────────────────────────────────────────────────────────────────────
feat_used = Choosen_model.feature_name()
X_feb     = df_feb_model[feat_used].copy()  # KeyError si falta alguna

# 5) Prepara matriz y predice
X_mat     = X_feb.to_numpy(dtype=np.float32)
best_it   = getattr(Choosen_model, "best_iteration", 1)
y_predlog = Choosen_model.predict(X_mat, num_iteration=best_it)

# ── decide si aplicar expm1 ───────────────────────────────────────
if sinlog:                                   # modelo SIN log
    y_pred_tn   = y_predlog                  # ya está en tn reales
    y_predlog   = np.log1p(y_predlog)        # (opcional) guardas el log
else:                                        # modelo CON log
    y_pred_tn   = np.expm1(np.clip(y_predlog, -20, 20))  # log → tn

# 6) Añade las predicciones a df_feb
df_feb["y_pred_log"] = y_predlog
df_feb["pred_tn"]    = y_pred_tn
# ────────────────────────────────────────────────────────────────────
# 7) (Opcional) Inserta de vuelta en el DataFrame completo df_febTuned
# ────────────────────────────────────────────────────────────────────
df_febTuned.loc[mask_feb, ["y_pred_log","pred_tn"]] = df_feb[["y_pred_log","pred_tn"]]

# ────────────────────────────────────────────────────────────────────
# 8) Vista rápida
# ────────────────────────────────────────────────────────────────────
print(df_feb[["periodo","product_id","y_pred_log","pred_tn"]].head(10))

import pandas as pd

# Constante de febrero
FEB = pd.to_datetime("2020-02-01")

# 1) Filtra solo las filas de febrero
df_submit = df_febTuned.loc[df_febTuned["periodo"] == FEB, ["product_id", "pred_tn"]].copy()

# 2) Renombra la columna de predicción al nombre que exige Kaggle (por ejemplo "tn")
df_submit = df_submit.rename(columns={"pred_tn": "tn"})

# 3) Exporta a CSV delimitado por comas
output_path = '/content/sample_data/df_kaggle_febrero_tuned_model.csv'
df_submit.to_csv(output_path, index=False)

print(f"✅ Archivo de submission generado en: {output_path}")

"""Exportacion final"""

import pandas as pd

# 1️⃣ – Renombra la columna `tn` del segundo dataframe
df_submit_renamed = df_submit.rename(columns={'tn': 'tn_feb'})

# 2️⃣ – Une los dos dataframes por `product_id`
df_merged = (
    df_preds
      .merge(df_submit_renamed, on='product_id', how='left')  # «left» conserva todas las filas de df_preds
      .loc[:, ['product_id', 'real_tn', 'pred_tn','tn_feb']]  # orden opcional de columnas
)

# 3️⃣ – Guarda el resultado en CSV (delimitado por comas)
output_path = '/content/sample_data/df_train+tuned_model.csv'
df_merged.to_csv(output_path, index=False, float_format='%.6f')  # float_format es opcional

print(f"✅ Archivo generado en: {output_path}")
