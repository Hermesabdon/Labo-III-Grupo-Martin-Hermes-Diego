# -*- coding: utf-8 -*-
"""lightGbm Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xNsDx6H_QwlWMtNHWzEN7t9723xLGxVP
"""

!pip install mlforecast
!pip install window_ops

###### Feature Engineering ######
import pandas as pd
import numpy as np
from window_ops.rolling import rolling_mean, rolling_max, rolling_min

###### Load the API  ######
import requests
import gzip
import io

###### Forecasting ######
from mlforecast import MLForecast
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from xgboost import XGBRegressor
import lightgbm as lgb



###### Visualize ######
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import numpy as np
from window_ops.rolling import rolling_mean, rolling_max, rolling_min
from sklearn.base import BaseEstimator, TransformerMixin # Import BaseEstimator and TransformerMixin
from sklearn.preprocessing import LabelEncoder # Import LabelEncoder

###### Load the API  ######
import requests
import gzip
import io

###### Forecasting ######
from mlforecast import MLForecast
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from xgboost import XGBRegressor
import lightgbm as lgb


###### Visualize ######
import matplotlib.pyplot as plt
import seaborn as sns

"""armamos funciones basicas"""

import numpy as np
import pandas as pd
import re
from sklearn.metrics import mean_absolute_error
import lightgbm as lgb
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import LabelEncoder
import requests
import gzip
import io
import matplotlib.pyplot as plt
import seaborn as sns


def calculate_error(forecast: np.ndarray, actual: np.ndarray) -> float:
    """
    Calcula el Total Forecast Error:

        ∑ |actual_i - forecast_i|
        -------------------------
           ∑ actual_i

    Parámetros
    ----------
    forecast : np.ndarray
        Array de valores pronosticados (ForecastSales).
    actual : np.ndarray
        Array de valores reales (ActualSales).

    Devuelve
    -------
    float
        El error total de pronóstico.
    """
    # Asegurarnos de que sean floats para evitar divisiones enteras
    forecast = forecast.astype(float)
    actual   = actual.astype(float)

    # Numerador: suma de errores absolutos
    numerador = np.sum(np.abs(actual - forecast))
    # Denominador: suma de ventas reales
    denominador = np.sum(actual)

    # Evitar división por cero
    if denominador == 0:
        raise ValueError("La suma de actual (denominador) es cero.")

    return numerador / denominador



def cargar_dataset(url, sep='\t', header='infer'):
    """
    Lee un archivo CSV comprimido con gzip desde una URL y lo carga en un DataFrame de Pandas.

    :param url: String con la URL del archivo .gz.
    :param sep: String delimitador para separar campos en el archivo CSV.
    :param header: Parámetro para determinar la fila que debe usarse como nombre de las columnas.
    :return: DataFrame de Pandas con los datos del archivo CSV.
    """
    # Realizar la petición HTTP para obtener el contenido del archivo
    response = requests.get(url)

    # Asegurarse de que la petición fue exitosa
    if response.status_code == 200:
        # Descomprimir el contenido en memoria y leer directamente en un DataFrame
        with io.BytesIO(response.content) as bytes_io:
            with gzip.open(bytes_io, 'rt') as read_file:
                df = pd.read_csv(read_file, sep=sep, header=header)
        return df
    else:
        raise IOError(f"Error al descargar el archivo: {response.status_code}")

def read_csv_from_url(url, sep='\t', header='infer'):
    response = requests.get(url)
    if response.status_code == 200:
        df = pd.read_csv(io.StringIO(response.content.decode('utf-8')), sep=sep, header=header)
        return df
    else:
        raise IOError(f"Error al descargar el archivo: {response.status_code}")

def plotear_grafico(df):
    # Convertir el 'periodo' a formato de fecha para mejor visualización en el gráfico
    df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')

    # Crear nuevamente el gráfico de línea con Seaborn
    plt.figure(figsize=(14, 7))
    line_plot = sns.lineplot(data=df, x='periodo', y='tn', marker='o', color='blue', linewidth=2.5)

    # Títulos y etiquetas
    line_plot.set_title('Evolución de Toneladas a lo largo del Tiempo', fontsize=16)
    line_plot.set_xlabel('Periodo', fontsize=14)
    line_plot.set_ylabel('Toneladas (tn)', fontsize=14)

    # Establecer el formato del eje x para mostrar todos los periodos
    line_plot.xaxis.set_major_locator(plt.MaxNLocator(len(df['periodo'])))
    line_plot.set_xticklabels(df['periodo'].dt.strftime('%Y-%m'), rotation=45, ha='right')

    # Mostrar el gráfico
    plt.tight_layout()
    plt.show()

def completar_periodos(df, fecha_inicio="2017-01-01", fecha_fin="2020-02-01"):
    """
    Devuelve un DataFrame con todas las combinaciones product_id × mes
    entre fecha_inicio y fecha_fin. Los campos que varían mes a mes se
    dejan en NaN (el modelo los completará después).
    """
    # 1) calendario mensual (primer día de mes)
    meses = pd.date_range(start=fecha_inicio, end=fecha_fin, freq="MS")

    # 2) producto cartesiano product_id × meses
    full_idx = pd.MultiIndex.from_product(
        [df["product_id"].unique(), meses],
        names=["product_id", "periodo"]
    )
    df_full = (df
               .set_index(["product_id", "periodo"])
               .reindex(full_idx)
               .reset_index())

    # 3) columnas estáticas que SÍ podemos rellenar
    cols_static = ["plan_precios_cuidados", "cat1", "cat2", "cat3", "brand", "sku_size"]
    df_full.sort_values(["product_id", "periodo"], inplace=True)
    df_full[cols_static] = (
        df_full.groupby("product_id")[cols_static]
               .ffill()
               .bfill()
    )

    # tn, cust_request_tn y futuros lags quedan en NaN
    return df_full



def generar_delta_lags_optimizado(df, prefijo_lag='lag_', prefijo_delta_lag='delta_lag_'):
    """
    Genera delta lags de manera más eficiente para una columna específica de un DataFrame de pandas.

    :param df: DataFrame original que ya contiene las columnas de lags.
    :param prefijo_lag: Prefijo utilizado para las columnas de lags en el DataFrame.
    :param prefijo_delta_lag: Prefijo utilizado para las nuevas columnas de delta lags.
    :return: DataFrame con las columnas de delta lags añadidas.
    """

    # Identificar todas las columnas de lags en el DataFrame
    columnas_lags = [col for col in df.columns if col.startswith(prefijo_lag)]

    # Generar las columnas de delta lags
    for i in range(len(columnas_lags) - 1):
        col_lag_actual = columnas_lags[i]
        col_lag_siguiente = columnas_lags[i + 1]
        df[prefijo_delta_lag + str(i + 1)] = df[col_lag_actual] - df[col_lag_siguiente]

    return df

# Re-creating the optimized lag generation function after the reset
def generar_lags_optimizado(df, nombre_col_original='tn', cant_lags=12):
    """
    Genera lags de manera más eficiente para una columna específica de un DataFrame de pandas.

    :param df: DataFrame original.
    :param nombre_col_original: Nombre de la columna sobre la cual se quieren calcular los lags.
    :param cant_lags: Cantidad de lags a generar.
    :return: DataFrame con las columnas de lags añadidas.
    """

    # Obtener una lista de productos únicos
    productos_unicos = df['product_id'].unique()

    # Preparar un contenedor para los DataFrames de lags
    lags_list = []

    # Iterar sobre cada producto único
    for producto in productos_unicos:
        # Filtrar el DataFrame por producto
        df_producto = df[df['product_id'] == producto].copy()

        # Generar lags para la columna deseada
        for lag in range(1, cant_lags + 1):
            df_producto[f'lag_{lag}'] = df_producto[nombre_col_original].shift(lag)

        # Añadir el DataFrame de lags a la lista
        lags_list.append(df_producto)

    # Concatenar todos los DataFrames de lags
    df_con_lags = pd.concat(lags_list).sort_index()

    return df_con_lags


def generar_lags_optimizadov2(df, col='tn', n_lags=12, prefijo='lag'):
    """
    Añade columnas lag_1 … lag_n_lags para `col`, por product_id,
    de forma vectorizada (sin bucles externos costosos).

    Parameters
    ----------
    df : pandas.DataFrame
    col : str
        Nombre de la columna sobre la que quieres los lags.
    n_lags : int
        Número de lags a generar.
    prefijo : str
        Prefijo de las nuevas columnas (lag, stock_t, etc.).

    Returns
    -------
    pandas.DataFrame
        El DataFrame original + columnas de lags.
    """
    out = df.copy()
    for k in range(1, n_lags + 1):
        out[f"{prefijo}_{k}"] = (
            out.groupby("product_id")[col].transform(lambda s: s.shift(k))
        )
    return out



def generar_medias_optimizado(df, nombre_col_original='tn', lista_medias_target=[2,3,4,6,10], prefijo_col_media='media'):
    """
    Genera las medias móviles de los últimos N meses de manera más eficiente para una columna específica de un DataFrame de pandas.

    :param df: DataFrame original.
    :param nombre_col_original: Nombre de la columna sobre la cual se quieren calcular las medias móviles.
    :param lista_medias_target: Lista de enteros que representan el número de meses para calcular la media móvil.
    :param prefijo_col_media: Prefijo para las nuevas columnas de medias móviles.
    :return: DataFrame con las columnas de medias móviles añadidas.
    """

    # Obtener una lista de productos únicos
    productos_unicos = df['product_id'].unique()

    # Preparar un contenedor para los DataFrames de medias móviles
    medias_list = []

    # Iterar sobre cada producto único
    for producto in productos_unicos:
        # Filtrar el DataFrame por producto
        df_producto = df[df['product_id'] == producto].copy()

        # Generar medias móviles para cada ventana especificada en lista_medias_target
        for ventana in lista_medias_target:
            col_media = f'{nombre_col_original}_{prefijo_col_media}_{ventana}'
            df_producto[col_media] = df_producto[nombre_col_original].rolling(window=ventana).mean().shift()

        # Añadir el DataFrame de medias móviles a la lista
        medias_list.append(df_producto)

    # Concatenar todos los DataFrames de medias móviles
    df_con_medias = pd.concat(medias_list).sort_index()

    return df_con_medias


class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.encoders = {}

    def fit(self, X, y=None):
        for column in X.columns:
            self.encoders[column] = LabelEncoder().fit(X[column])
        return self

    def transform(self, X):
        X_encoded = X.copy()
        for column in X.columns:
            X_encoded[column] = self.encoders[column].transform(X[column])
        return X_encoded

    def inverse_transform(self, X):
        X_decoded = X.copy()
        for column in X.columns:
            X_decoded[column] = self.encoders[column].inverse_transform(X[column])
        return X_decoded


# ────────────────────────────────────────────────────────────────
# 1️⃣  LAGS  (prefijo inteligente)
# ────────────────────────────────────────────────────────────────
def generar_lags_optimizadov2(df, col='tn', n_lags=12, prefijo='lag'):
    """
    Crea columnas lag_k para `col` por product_id.
    ✔ No añade subrayado duplicado:
        prefijo = 'lag'      → lag_1
        prefijo = 'stock_t'  → stock_t1
    """
    sep = "_" if not prefijo.endswith(("_", "t")) else ""   # evita stock_t_1
    out = df.copy()
    for k in range(1, n_lags + 1):
        out[f"{prefijo}{sep}{k}"] = (
            out.groupby("product_id")[col].transform(lambda s: s.shift(k))
        )
    return out

# ────────────────────────────────────────────────────────────────
# 2️⃣  DELTA-LAGS  (regex robusto)
# ────────────────────────────────────────────────────────────────
def generar_delta_lags_optimizado_v2(df, base_prefijo="lag_", prefijo_out="delta_lag_"):
    """
    Delta_k = lag_k − lag_{k+1}  (funciona con 'lag_1' o 'stock_t1').
    """
    out = df.copy()
    cols = sorted(
        [c for c in out.columns if c.startswith(base_prefijo)],
        key=lambda x: int(re.search(r'(\d+)$', x).group(1))
    )
    for i in range(len(cols) - 1):
        k = int(re.search(r'(\d+)$', cols[i]).group(1))
        out[f"{prefijo_out}{k}"] = out[cols[i]] - out[cols[i + 1]]
    return out

# ────────────────────────────────────────────────────────────────
# 3️⃣  MEDIAS  (vectorizado)
# ────────────────────────────────────────────────────────────────
def generar_medias_optimizado_v2(df, col, ventanas=(2,3,4,6,10), prefijo="media"):
    """
    media_k = mean(col, window=k) desplazado 1 periodo.
    """
    out = df.copy()
    g = out.groupby("product_id")[col]
    for w in ventanas:
        out[f"{col}_{prefijo}_{w}"] = (
            g.rolling(window=w, min_periods=1).mean()
              .shift()
              .reset_index(level=0, drop=True)
        )
    return out

# ──────────────────────────────────────────────────────────────
# Función utilitaria
# ──────────────────────────────────────────────────────────────
def eval_producto(
    model,
    df_test,                # DataFrame de diciembre
    product_ids,            # int o lista de ints
    id_col="product_id",
    target_col="tn",        # en log1p
    num_iter=None
):
    """
    Devuelve DataFrame con métricas MAE / MAPE para uno o varios productos.
    """
    if isinstance(product_ids, int):
        product_ids = [product_ids]

    # 1) prepara X_test y predicciones en log
    X_test = prep_strict(df_test)
    y_pred_log = model.predict(X_test, num_iteration=num_iter)

    tmp = df_test.copy()
    tmp["y_pred_log"] = y_pred_log

    # 2) filtra productos seleccionados
    mask = tmp[id_col].isin(product_ids)
    pe   = tmp[mask].copy()

    # 3) pasa a escala real
    pe["y_true_lin"] = np.expm1(pe[target_col])
    pe["y_pred_lin"] = np.expm1(pe["y_pred_log"])
    pe["abs_err"] = np.abs(pe["y_true_lin"] - pe["y_pred_lin"])
    pe["ape"] = np.where(
        pe["y_true_lin"] != 0,
        pe["abs_err"] / pe["y_true_lin"],
        np.nan
    )

    # 4) métricas agregadas por producto
    resumen = (
        pe.groupby(id_col)
          .agg(
              MAE_lin  = ("abs_err", "mean"),
              MAPE_lin = ("ape",    "mean"),
              N_obs    = ("abs_err","size")
          )
          .reset_index()
    )

    return resumen, pe



# ------------------------------------------------------------------
# 3) ENTRENAMIENTO  (con pesos WMAPE)
# ------------------------------------------------------------------
def train_lgb(X_train, y_train, X_valid, y_valid, params,
              num_rounds=3000, early_stop=100, name='model'):

    # Pesos = toneladas reales  (aproxima WMAPE)
    w_train = np.expm1(y_train) if USE_LOG_TARGET else y_train
    w_valid = np.expm1(y_valid) if USE_LOG_TARGET else y_valid

    dtrain = lgb.Dataset(X_train, y_train,
                         weight=w_train,
                         categorical_feature=cat_feats)
    dvalid = lgb.Dataset(X_valid, y_valid,
                         weight=w_valid,
                         categorical_feature=cat_feats)

    booster = lgb.train(
        params,
        dtrain,
        num_boost_round=num_rounds,
        valid_sets=[dvalid],
        valid_names=['valid'],
        callbacks=[
            lgb.early_stopping(early_stop, verbose=False),
            lgb.log_evaluation(200)
        ]
    )
    mae_val = mean_absolute_error(
        y_valid, booster.predict(X_valid, num_iteration=booster.best_iteration)
    )
    print(f"▶ {name} | MAE valid (log): {mae_val:.4f} | best_iter: {booster.best_iteration}")
    return booster

# ------------------------------------------------------------------
# Métrica personalizada
# ------------------------------------------------------------------
def mape_safe(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:
    mask = y_true != 0
    if not np.any(mask):
        return np.nan
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / (y_true[mask] + eps)))

def wmape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:
    return np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + eps)


# Métrica WMAPE para LightGBM
def lgb_wmape_eval(y_pred, dataset):
    # y_pred y y_train están en log → pasamos a toneladas
    y_pred_tn = np.expm1(y_pred) if USE_LOG_TARGET else y_pred
    y_true_tn = np.expm1(dataset.get_label()) if USE_LOG_TARGET else dataset.get_label()

    # si usas pesos = tn reales, tómalos de ahí; si no, usa y_true_tn
    weight = dataset.get_weight()
    if weight is None:
        weight = y_true_tn

    wmape = np.sum(np.abs(y_true_tn - y_pred_tn)) / (np.sum(weight) + 1e-8)
    return "WMAPE", wmape, False        # False → cuanto más bajo, mejor

def feature_pipeline(df: pd.DataFrame) -> pd.DataFrame:
    """
    Recibe un df que YA incluye stock_t0,
    y devuelve el df con todas las features.
    """
    df = df.copy()

    # 1) Calendar
    df["periodo"] = pd.to_datetime(df["periodo"]) # Ensure periodo is datetime
    df["month"]     = df["periodo"].dt.month.astype("int8")
    df["quarter"]   = df["periodo"].dt.quarter.astype("int8")
    df["month_sin"] = np.sin(2*np.pi*df["month"]/12)
    df["month_cos"] = np.cos(2*np.pi*df["month"]/12)

    # 2) Asegura stock_t0 (si no está, lo crea con NaN)
    if "stock_t0" not in df.columns:
        df["stock_t0"] = np.nan

    # 3) Lags de tn (1–24)
    df = generar_lags_optimizadov2(df, col="tn", n_lags=24)

    # 4) Deltas y medias sobre tn
    df = generar_delta_lags_optimizado_v2(df)
    df = generar_medias_optimizado_v2(df, col="tn", ventanas=(2,3,4,6,10))

    # 5) Features de stock
    # Ensure 'lag_1' is created before accessing it
    df["flag_stock_disp"] = (~df["stock_t0"].isna()).astype("int8")
    df["ratio_tn_stock"]  = (
        df["lag_1"] / (df["stock_t0"] + 1e-3) # Access 'lag_1' after it's generated
    ).where(df["flag_stock_disp"] == 1, np.nan)

    # 6) YoY (lag 12, delta_yoy, ratio_yoy)
    df = df.sort_values(["product_id","periodo"])

    # Ensure 'lag_1' and 'lag_13' are created before accessing them
    df["delta_yoy"] = df["lag_1"] - df["lag_13"] # Access 'lag_1' and 'lag_13' after they are generated
    df["ratio_yoy"] = df["lag_1"] / (df["lag_13"] + 1e-6) # Access 'lag_1' and 'lag_13' after they are generated

    return df



# URL del archivo .gz
url_tb_sellout_02 = 'https://storage.googleapis.com/open-courses/austral2025-af91/labo3v/sell-in.txt.gz'
url_tb_stocks_02 = 'https://storage.googleapis.com/open-courses/austral2025-af91/labo3v/tb_stocks.txt'
url_tb_productos_02 = 'https://storage.googleapis.com/open-courses/austral2025-af91/labo3v/tb_productos.txt'





# Uso de la función
df_tb_sellout_02 = cargar_dataset(url_tb_sellout_02)
df_tb_stocks_02= read_csv_from_url(url_tb_stocks_02)
df_tb_productos_02= read_csv_from_url(url_tb_productos_02)

df_tb_sellout_02.head(50)

productospredecir=pd.read_csv("/content/sample_data/product_id_apredecir201912.txt",sep="\t",header=0)

df_tb_sellout_02['periodo'].unique()

# Juntamos los datasets Sellout con el maestro de productos
df_sellout_with_cat = pd.merge(df_tb_sellout_02, df_tb_productos_02[['product_id', 'cat1', 'cat2', 'cat3', 'brand', 'sku_size']], on='product_id', how='left')
# Agrupamos por categorías de productos
df_aggregated = df_sellout_with_cat.groupby(['periodo', 'product_id', 'plan_precios_cuidados',
                                             'cat1', 'cat2', 'cat3', 'brand', 'sku_size']).agg({'cust_request_tn': 'sum','tn': 'sum'}).reset_index()
# 1) Asegúrate de que ambas columnas product_id sean del mismo tipo
df_aggregated["product_id"]      = df_aggregated["product_id"].astype(int)
productospredecir["product_id"] = productospredecir["product_id"].astype(int)

# 2) El join: basta con quedarnos con los IDs que aparecen en la lista
df_filtrado = (
    df_aggregated.merge(
        productospredecir[["product_id"]].drop_duplicates(),  # solo la llave
        on="product_id",
        how="inner"                                           # ← inner join
    )
)

print("Filas antes :", len(df_aggregated))
print("Filas después:", len(df_filtrado))

df = df_filtrado.copy()                        # tu tabla de origen
df["periodo"] = pd.to_datetime(df["periodo"].astype(str), format="%Y%m")
df_full=df

"""### Feature engineering - se transforma a log y se hace primero los lags y deltas sobre las variables crudas como vienen

**Armamos un product id ficticio para correr las escalas ahora lo ignoramos**
"""

df_subset=df_full

# Crear un nuevo DataFrame con los mismos periodos que df_subset
periodos = df_subset['periodo'].unique()  # Extraer los periodos únicos
datos_ficticios = {
    'periodo': periodos,
    'product_id': 99999,  # ID del producto ficticio
    'plan_precios_cuidados': 0.0,
    'cat1': 'HC',
    'cat2': 'Artificial',
    'cat3': 'Artificial',
    'brand': 'Producto_artificial',
    'sku_size': 3000.0,
    # Inicializamos cust_request_tn y tn con ceros, los actualizaremos luego
    'cust_request_tn': [0] * len(periodos),
    'tn': [0] * len(periodos)
}

df_ficticio = pd.DataFrame(datos_ficticios)

# Ahora, actualizamos 'cust_request_tn' y 'tn' con 10 veces los valores del producto 20001
# Asumimos que 'product_id' es único por periodo en df_subset
for periodo in periodos:
    # Encuentra los valores para el producto 20001 en el periodo actual
    valores_producto = df_subset[(df_subset['periodo'] == periodo) & (df_subset['product_id'] == 20001)]
    if not valores_producto.empty:
        df_ficticio.loc[df_ficticio['periodo'] == periodo, 'cust_request_tn'] = valores_producto['cust_request_tn'].values[0] * 3
        df_ficticio.loc[df_ficticio['periodo'] == periodo, 'tn'] = valores_producto['tn'].values[0] * 3

# Concatenar el DataFrame ficticio con el original
df_subset_con_ficticio = pd.concat([df_subset, df_ficticio], ignore_index=True)

"""***aplicamos Lags a todo , stocks - tn - y le hacemos tambien promedios y delta lags, lo que paso es que cuando meti el stock , mejoro pero despues empeor, pueden ser tres cos"""

df_final=df_full

# ───────────────────────────────────────────────────────────
# 🔖 FEATURE: Antigüedad en meses desde primera aparición
# ───────────────────────────────────────────────────────────
# Primero: calculamos la fecha mínima (primera aparición)
fecha_inicio = df_final.groupby('product_id')['periodo'].min().reset_index()
fecha_inicio.columns = ['product_id', 'fecha_primera_aparicion']

# Añadimos esta fecha original al DataFrame principal
df_final = df_final.merge(fecha_inicio, on='product_id', how='left')

# Creamos la variable "antiguedad_meses"
df_final['antiguedad_meses'] = (
    (df_final['periodo'].dt.year - df_final['fecha_primera_aparicion'].dt.year) * 12 +
    (df_final['periodo'].dt.month - df_final['fecha_primera_aparicion'].dt.month)
).astype(int)

# Verificamos rápido
print(df_final[['product_id', 'periodo', 'fecha_primera_aparicion', 'antiguedad_meses']].head(10))

df_final.drop(columns=['fecha_primera_aparicion'], inplace=True)

df_final=df_final


# ------------------------------------------------------------------
# 0)  BASE: ventas lineales
# ------------------------------------------------------------------
feats = df_final.copy()
feats["periodo"] = pd.to_datetime(feats["periodo"], format="%Y%m")

# ------------------------------------------------------------------
# 1) MERGE con stock desplazado +1 mes (stock_t0)
# ------------------------------------------------------------------
stock = (
    df_tb_stocks_02.copy()
        .assign(periodo=lambda d:
                pd.to_datetime(d["periodo"].astype(str), format="%Y%m")
                + pd.offsets.MonthEnd(0)      # 201810 → 2018-10-31
                + pd.offsets.MonthBegin(1))   #        → 2018-11-01
        .rename(columns={"stock_final": "stock_t0"})
)

featsandStock = feats.merge(
    stock[["product_id", "periodo", "stock_t0"]],
    on=["product_id", "periodo"],
    how="left"
)

featsandStock.columns

# ────────────────────────────────────────────────────
# 1) Generas todas las features de una sola vez
# ────────────────────────────────────────────────────
featspipelined = feature_pipeline(featsandStock)   # raw_df incluye periodo, product_id, tn, etc.

# ────────────────────────────────────────────────────
# 2) Target y sample weights
# ────────────────────────────────────────────────────
featspipelined["tn_log"] = np.log1p(featspipelined["tn"])
y = featspipelined["tn_log"]
weights = np.expm1(y)

# ────────────────────────────────────────────────────
# 3) Filtrado histórico de entrenamiento
# ────────────────────────────────────────────────────
mask = featspipelined["periodo"].between("2018-01-01","2019-12-01")
readyforModel = featspipelined.loc[mask].copy()

readyforModel.tail()

"""***Modelo listo para correr***

***Vista Resumida***
"""

# --- Info y resumen de readyforModel -----------------------------------
import pandas as pd
import numpy as np
import io, sys

# A) info() en texto
buf = io.StringIO()
readyforModel.info(buf=buf)
print(">>>  readyforModel.info()")
print(buf.getvalue())

# B) NAs por columna, ordenados
print("\n>>>  Top 30 columnas con más NAs")
na = readyforModel.isna().sum().sort_values(ascending=False)
display(na.head(30))

# C) Estadísticos básicos de las 15 primeras numéricas
print("\n>>>  describe() primeras numéricas")
display(readyforModel.select_dtypes(include=[np.number]).iloc[:, :15].describe().T)

# D) Contar cuántas columnas usan tn_t (terminan en '_t0')
cols_t0 = [c for c in readyforModel.columns if '_t' in c and c.endswith('0')]
print(f"\n>>>  Columnas basadas en tn_t0: {len(cols_t0)}")
print(cols_t0[:15])   # muestra las primeras 15

"""# Armamos Multiples Modelos con diferentes complejidades

**Parte nueva optimizacion y agrupacion de intentos**

1- lightGb base tuned y tweedie
"""

# ╔══════════════════════════════════════════════════════════╗
# 0)  IMPORTS, CONSTANTES Y UTILIDADES COMUNES
# ╚══════════════════════════════════════════════════════════╝
import numpy as np, pandas as pd, lightgbm as lgb, optuna
from sklearn.metrics import mean_absolute_error

DROP_COLS_NOW = ['tn', 'tn_log', 'periodo', 'cust_request_tn']
cat_feats     = ['cat1','cat2','cat3','brand','month','quarter']
seed          = 42

# ── prep básico (sin fugas) ───────────────────────────────────────
def prep_strict(df: pd.DataFrame) -> pd.DataFrame:
    X = df.drop(columns=DROP_COLS_NOW, errors="ignore").copy()
    X.drop(columns=X.select_dtypes('datetime64[ns]').columns, inplace=True)
    for c in cat_feats:
        if c in X: X[c] = X[c].astype('category')
    return X

import pandas as pd

# ── Parche Único: redefine build_cat_levels y encode_cats ──

# 1) build_cat_levels acepta ahora sólo (df_train),
#    usando tu lista global cat_feats
def build_cat_levels(df_train):
    return {
        c: pd.Categorical(df_train[c]).categories
        for c in cat_feats
        if c in df_train.columns
    }

# 2) encode_cats redirige a tu versión unificada
def encode_cats(df, cat_levels):
    """
    Sobrescribe la encode_cats antigua.
    Internamente usa encode_cats_unified con cat_feats global.
    """
    out = df.copy()
    for c in cat_feats:
        if c in out.columns and c in cat_levels:
            out[c] = (
                pd.Categorical(out[c], categories=cat_levels[c])
                  .codes
                  .astype("int32")
            )
    return out




# ── métricas auxiliares ──────────────────────────────────────────
def wmape(y_true, y_pred, eps=1e-8):
    return np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + eps)

def mape_safe(y_true, y_pred, eps=1e-9):
    return np.mean(np.abs((y_true - y_pred) /
                          np.maximum(np.abs(y_true), eps)))

def to_tn(raw_pred, use_log, clip=20):
    if use_log:
        raw_pred = np.clip(raw_pred, -clip, clip)
        return np.expm1(raw_pred)
    return raw_pred

# ── función estándar de entrenamiento LightGBM ───────────────────
def train_lgb(Xtr, ytr, Xval, yval, params, num_rounds, early_stop, name):
    dtr  = lgb.Dataset(Xtr,  ytr)
    dval = lgb.Dataset(Xval, yval)
    return lgb.train(
        params,
        dtr,
        num_boost_round=num_rounds,
        valid_sets=[dval],
        valid_names=['valid'],
        callbacks=[lgb.early_stopping(early_stop, verbose=False),
                   lgb.log_evaluation(200)],
        )

# ╔══════════════════════════════════════════════════════════╗
# 1)  SPLITS (solo una vez)  ➜  train_oct / valid_nov / test_dec
# ╚══════════════════════════════════════════════════════════╝
train_oct = readyforModel[readyforModel['periodo'] <= '2019-10-01']
valid_nov = readyforModel[readyforModel['periodo'] == '2019-11-01']
test_dec  = readyforModel[readyforModel['periodo'] == '2019-12-01']

X_tr_raw, X_val_raw, X_test_raw = map(prep_strict,
                                      [train_oct, valid_nov, test_dec])
cat_levels = build_cat_levels(X_tr_raw)

X_tr, X_val, X_test = map(lambda df: encode_cats(df, cat_levels),
                          [X_tr_raw, X_val_raw, X_test_raw])
X_tr_mat, X_val_mat, X_test_mat = [m.to_numpy(np.float32)
                                   for m in [X_tr, X_val, X_test]]

y_true_tn = test_dec['tn']

# ╔══════════════════════════════════════════════════════════╗
# 2)  HP GLOBALS (misma convención de nombres)
# ╚══════════════════════════════════════════════════════════╝
params_base = dict(objective='regression', metric='mae',
                   learning_rate=0.1, num_leaves=50,
                   seed=seed, deterministic=True, nthread=1)

params_tuned = dict(params_base,
    learning_rate=0.08813259224734658, num_leaves=79,
    min_child_samples=100,
    feature_fraction=0.9833497436436693,
    bagging_fraction=0.9951622648438583,
    bagging_freq=0, lambda_l2=0.3939212978268534, verbose=-1)

# ╔══════════════════════════════════════════════════════════╗
# 3)  PIPELINE PARA CADA ESCALA  (“log” y “sinLog”)
#     ==> conserva naming convention original
# ╚══════════════════════════════════════════════════════════╝
def run_pipeline(use_log: bool):
    global base_model, base_model_sinLog
    global tuned_model, tuned_model_sinLog
    global tweed_opt_model, tweed_opt_model_sinLog

    TARGET_COL = 'tn_log' if use_log else 'tn'
    y_tr   = train_oct[TARGET_COL]
    y_val  = valid_nov[TARGET_COL]
    tag_suffix = '' if use_log else '_sinLog'

    # ── 1) BASE ─────────────────────────────────────────────
    base_model = train_lgb(
        X_tr_mat, y_tr, X_val_mat, y_val,
        params_base, num_rounds=1000, early_stop=50, name='BASE'
    )
    globals()[f'base_model{tag_suffix}'] = base_model

    # Debug DIC-19 para BASE
    df_dec = december_error_df_safe(
        encode_cats(test_dec.copy(), cat_levels),
        base_model,
        sinlog=not use_log,
        tag=f"BASE{tag_suffix}"
    )
    globals()[f'output_train_BASE{tag_suffix}'] = df_dec[
        ["product_id", "real_tn", f"pred_tn_BASE{tag_suffix}"]
    ]

    # ── 2) TUNED ────────────────────────────────────────────
    tuned_model = train_lgb(
        X_tr_mat, y_tr, X_val_mat, y_val,
        params_tuned, num_rounds=4000, early_stop=200, name='TUNED'
    )
    globals()[f'tuned_model{tag_suffix}'] = tuned_model

    # Debug DIC-19 para TUNED
    df_dec = december_error_df_safe(
        encode_cats(test_dec.copy(), cat_levels),
        tuned_model,
        sinlog=not use_log,
        tag=f"TUNED{tag_suffix}"
    )
    globals()[f'output_train_TUNED{tag_suffix}'] = df_dec[
        ["product_id", "real_tn", f"pred_tn_TUNED{tag_suffix}"]
    ]

    # ── 3) TWEEDIE LOOP ─────────────────────────────────────
    best_w, best_p, best_tw = 1e9, None, None
    for p in [1.1, 1.2, 1.3, 1.4, 1.5]:
        p_params = dict(params_base, objective='tweedie', tweedie_variance_power=p)
        mdl = train_lgb(
            X_tr_mat, y_tr, X_val_mat, y_val,
            p_params, num_rounds=2000, early_stop=200, name=f'TW_p{p}'
        )
        pred = to_tn(mdl.predict(X_val_mat, num_iteration=mdl.best_iteration), use_log)
        wm   = wmape(to_tn(y_val, use_log), pred)
        if wm < best_w:
            best_w, best_p, best_tw = wm, p, mdl

    tweed_opt_model = best_tw
    globals()[f'tweed_opt_model{tag_suffix}'] = tweed_opt_model

    # Debug DIC-19 para TWEED
    tag = f"TWEED_{best_p:.1f}{tag_suffix}"
    df_dec = december_error_df_safe(
        encode_cats(test_dec.copy(), cat_levels),
        tweed_opt_model,
        sinlog=not use_log,
        tag=tag
    )
    globals()[f'output_train_{tag}'] = df_dec[
        ["product_id", "real_tn", f"pred_tn_{tag}"]
    ]

    # ── 4) Impresión de métricas finales ─────────────────────
    print(f"\n📊 Resultados DIC‑19  (use_log={use_log})")
    for tag, mdl in [
        ("BASE",  base_model),
        ("TUNED", tuned_model),
        (f"TWEED_{best_p:.1f}", tweed_opt_model)
    ]:
        raw = mdl.predict(X_test_mat, num_iteration=mdl.best_iteration)
        pred_tn = to_tn(raw, use_log)
        mae  = mean_absolute_error(y_true_tn, pred_tn)
        wmp  = wmape(y_true_tn, pred_tn)
        print(f"{tag:>14} | MAE {mae:,.2f} | WMAPE {wmp:.2%}")

run_pipeline(use_log=False)  # genera base_model_sinLog, tuned_model_sinLog, tweed_opt_model_sinLog

# -----------------------------------------------------------
# 1)  Elige si quieres UN modelo o TODOS
# -----------------------------------------------------------
# 1‑a)  ►  Solo un modelo (ej.: base_model_sinLog)
chosen_tag   = "BASE_SINLOG"          # cambia a TUNED, TWEED, etc.
chosen_model = globals()[f"{'base_model' if 'BASE' in chosen_tag else 'tuned_model' if 'TUNED' in chosen_tag else 'tweed_opt_model'}" + ("_sinLog" if "SINLOG" in chosen_tag else "")]
sinlog_flag  = "SINLOG" in chosen_tag

# ╔══════════════════════════════════════════════════════════╗
# 4)  EJECUCIÓN
# ╚══════════════════════════════════════════════════════════╝
run_pipeline(use_log=True)   # genera base_model, tuned_model, tweed_opt_model

"""Grouper optuna tweedie"""

# ╔══════════════════════════════════════════════════════════╗
# 0) REQUISITOS:  X_tr_mat, X_val_mat, X_tr, X_val, cat_levels,
#                 to_tn(), wmape(), train_lgb(), params_base
#                 …ya existen gracias al pipeline previo
# ╚══════════════════════════════════════════════════════════╝

def optuna_tweedie(use_log: bool,
                   n_trials: int = 30,
                   seed: int = 42):
    """
    Devuelve (booster, best_params, best_iter) optimizados vía Optuna.
    * Crea variables globales:
        - final_model           (para use_log=True)
        - final_model_sinLog    (para use_log=False)
        - output_train_OPT_FINAL_LOG     o SINLOG
    """
    TARGET = 'tn_log' if use_log else 'tn'
    y_tr, y_val = train_oct[TARGET], valid_nov[TARGET]

    def objective(trial):
        params = params_base | {
            "objective":              "tweedie",
            "metric":                 "mae",
            "seed":                   seed,
            "deterministic":          True,
            "tweedie_variance_power": trial.suggest_float("p", 1.05, 1.25, step=0.01),
            "learning_rate":          trial.suggest_float("lr", 0.015, 0.05, log=True),
            "num_leaves":             trial.suggest_int("num_leaves", 16, 96, step=8),
            "min_child_samples":      trial.suggest_int("min_child_samples", 120, 250, step=10),
            "feature_fraction":       trial.suggest_float("feature_fraction", 0.55, 0.85),
            "bagging_fraction":       trial.suggest_float("bagging_fraction", 0.55, 0.85),
            "lambda_l2":              trial.suggest_float("lambda_l2", 0.0, 1.0),
        }
        params["bagging_freq"] = 1 if params["bagging_fraction"] < 1.0 else 0

        booster = train_lgb(
            X_tr_mat, y_tr, X_val_mat, y_val,
            params, num_rounds=2000, early_stop=200,
            name=f"opt-{trial.number}"
        )

        trial.set_user_attr("best_iteration", booster.best_iteration)

        pred_val = to_tn(booster.predict(
                          X_val_mat, num_iteration=booster.best_iteration),
                          use_log)
        true_val = to_tn(y_val, use_log)
        return wmape(true_val, pred_val)

    study = optuna.create_study(direction="minimize",
                                sampler=optuna.samplers.TPESampler(seed=seed))
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

    # ── re‑entrena con train+valid ─────────────────────────────────
    best_iter   = study.best_trial.user_attrs["best_iteration"]
    best_params = params_base | study.best_params
    best_params["objective"] = "tweedie"

    X_trval_mat = np.concatenate([X_tr_mat, X_val_mat], axis=0)
    y_trval     = pd.concat([y_tr, y_val])

    booster_final = train_lgb(
        X_trval_mat, y_trval,
        X_val_mat,   y_val,        # sólo para callbacks
        best_params,
        num_rounds=best_iter,
        early_stop=200,
        name=f"OPT_FINAL_{'LOG' if use_log else 'SINLOG'}"
    )

    # ── guarda con el nombre correcto ─────────────────────────────
    global final_model, final_model_sinLog
    if use_log:
        final_model = booster_final
    else:
        final_model_sinLog = booster_final

    # ── NUEVO: genera DataFrame de predicciones DIC-19 para el modelo final Optuna
    tag = f"OPT_FINAL_{'LOG' if use_log else 'SINLOG'}"
    # codifica test_dec con tu encoder unificado
    test_dec_num = encode_cats(test_dec.copy(), cat_levels)
    df_out = december_error_df_safe(
        test_dec_num,
        booster_final,
        sinlog=(not use_log),
        tag=tag
    )[['product_id', 'real_tn', f'pred_tn_{tag}']]
    # guarda en variable global
    globals()[f"output_train_{tag}"] = df_out

    print("\n✅ Optuna terminado  |  use_log =", use_log)
    print("   best WMAPE :", study.best_value)
    print("   best params:", study.best_params)
    print("   best_iter  :", best_iter)
    print(f"🛠️ DataFrame de entrenamiento DIC-19 guardado en `output_train_{tag}`")

    return booster_final, study.best_params, best_iter



# ╔══════════════════════════════════════════════════════════╗
# 1)  LLAMADAS (log  +  sin‑log)
# ╚══════════════════════════════════════════════════════════╝
booster_log,    best_params_log,    best_iter_log    = optuna_tweedie(True)
booster_sinlog, best_params_sinlog, best_iter_sinlog = optuna_tweedie(False)

# ── comprobación rápida sobre Diciembre‑19 ───────────────────────
for tag, mdl, flag in [("OPT_LOG",    final_model,         True),
                       ("OPT_SINLOG", final_model_sinLog,  False)]:
    pred = to_tn(mdl.predict(X_test_mat, num_iteration=mdl.best_iteration),
                 flag)
    mae  = mean_absolute_error(y_true_tn, pred)
    wmp  = wmape(y_true_tn, pred)
    print(f"{tag:>11} | MAE {mae:,.2f} | WMAPE {wmp:.2%}")

"""Creamos Enero para todos los modelos"""

# ╔══════════════════════════════════════════════════════════╗
# 2)  build_january_predictions
#     • devuelve: df_pred_num (enero codificado + pred_tn_*)
#                 df_pred_raw (enero en formato «original» + pred_tn_*)
# ╚══════════════════════════════════════════════════════════╝
# ───────────────────────────────────────────────────────────────────
# ACTUALIZACIÓN: build_january_predictions adaptada a la nueva encode_cats
# ───────────────────────────────────────────────────────────────────
def build_january_predictions(
        df_final, df_tb_stocks_02,
        feature_pipeline,
        model_specs,             # [(tag, booster, sinlog), …]
        cat_levels,
        ENE: str = "2020-01-01"):

    # A. crea registro enero‑20 a partir de dic‑19
    base_cols = df_final.columns.difference(["tn", "cust_request_tn"])
    jan_proto = (
        df_final.loc[df_final["periodo"] == "2019-12-01", base_cols]
          .assign(periodo=pd.to_datetime(ENE),
                  tn=np.nan, cust_request_tn=np.nan,
                  antiguedad_meses=lambda d: d["antiguedad_meses"] + 1)
    )
    df_ene = (pd.concat([df_final, jan_proto], ignore_index=True)
                .assign(periodo=lambda d: pd.to_datetime(d["periodo"]))
              )

    # B. merge con stock desplazado +1 mes
    stock = (df_tb_stocks_02.copy()
               .assign(periodo=lambda d:
                       pd.to_datetime(d["periodo"].astype(str), format="%Y%m")
                       + pd.offsets.MonthEnd(0) + pd.offsets.MonthBegin(1))
               .rename(columns={"stock_final": "stock_t0"}))

    df_ene = df_ene.merge(
        stock[["product_id", "periodo", "stock_t0"]],
        on=["product_id", "periodo"], how="left")

    # ► df_ene  ⟶ formato «RAW»  (categorías intactas)
    df_ene_raw = df_ene.copy()

    # C. feature engineering y codificación para el modelo
    df_ene_tuned = feature_pipeline(df_ene)
    # aquí usamos la nueva firma: solo df y cat_levels
    df_ene_num   = encode_cats(df_ene_tuned, cat_levels)

    # D. filtra enero‑20 y elimina columnas target/leak
    jan_mask     = df_ene_num["periodo"] == pd.to_datetime(ENE)
    df_jan_input = df_ene_num.loc[jan_mask].drop(columns=DROP_COLS_NOW,
                                                 errors="ignore")

    # E. loop de modelos → añade pred_tn_<TAG>
    for tag, booster, sinlog in model_specs:
        feat_used = booster.feature_name()

        if set(feat_used).issubset(df_jan_input.columns):
            X_mat = df_jan_input[feat_used].to_numpy(np.float32)
        else:
            X_mat = df_jan_input.iloc[:, :len(feat_used)].to_numpy(np.float32)

        raw     = booster.predict(X_mat, num_iteration=booster.best_iteration)
        pred_tn = raw if sinlog else np.expm1(np.clip(raw, -20, 20))

        df_ene_num.loc[jan_mask, f"pred_tn_{tag}"] = pred_tn
        df_ene_raw.loc[jan_mask, f"pred_tn_{tag}"] = pred_tn

    return (
        df_ene_num.loc[jan_mask].reset_index(drop=True),
        df_ene_raw.loc[jan_mask].reset_index(drop=True)
    )


# ╔══════════════════════════════════════════════════════════╗
# 3)  make_roll_forward_dfs
#     • toma df_pred_raw (enero «original»+pred)
#     • devuelve dict {tag: dataframe_original_con_tn_actualizado}
# ╚══════════════════════════════════════════════════════════╝
def make_roll_forward_dfs(df_pred_raw, model_tags, ene_date="2020-01-01"):
    dfs, ene_mask = {}, df_pred_raw["periodo"] == pd.to_datetime(ene_date)

    for tag in model_tags:
        df_tmp = df_pred_raw.copy()

        # sobrescribe tn en enero‑20 con la predicción del modelo
        df_tmp.loc[ene_mask, "tn"] = df_tmp.loc[ene_mask,
                                                f"pred_tn_{tag}"].values

        # elimina columnas auxiliares pred_tn_*
        df_tmp.drop(columns=[c for c in df_tmp.columns
                             if c.startswith("pred_tn_")], inplace=True)

        dfs[tag] = df_tmp

    return dfs

# 1) especificación de modelos
model_specs = [
    ("BASE",          base_model,              False),
    ("TUNED",         tuned_model,             False),
    ("TWEED",         tweed_opt_model,         False),
    ("FINAL",         final_model,             False),
    ("FINAL_SINLOG",  final_model_sinLog,      False),
    ("BASE_SINLOG",   base_model_sinLog,       True),
    ("TUNED_SINLOG",  tuned_model_sinLog,      True),
    ("TWEED_SINLOG",  tweed_opt_model_sinLog,  True)
]
model_tags = [m[0] for m in model_specs]

# 2) genera enero‑20 con TODAS las predicciones
df_jan_num, df_jan_raw = build_january_predictions(
    df_final,          # dataset hasta dic‑19
    df_tb_stocks_02,   # stock table
    feature_pipeline,  # tu función de features
    model_specs,
    cat_levels         # dict niveles categóricos del TRAIN
)

# 3) crea un «fake dataset» (formato original) para CADA modelo
roll_dfs = make_roll_forward_dfs(df_jan_raw, model_tags)
print("Datasets disponibles en roll_dfs →", list(roll_dfs.keys()))

# 4) ejemplo de uso: dataset listo para procesar febrero con TUNED
df_para_feb_con_TUNED = roll_dfs["TUNED"]

print("Columnas →", df_para_feb_con_TUNED.columns.tolist()[:10], "…")
print(df_para_feb_con_TUNED.head())

# ------------------------------------------------------------------
# df_final  → histórico original hasta diciembre‑19
# roll_dfs  → {tag: df_solo_enero_20_con_tn_predicho}
# ------------------------------------------------------------------

hist_dfs = {}                        # dict de salida

for tag, df_jan in roll_dfs.items():
    # asegura MISMAS columnas y mismo orden que df_final
    df_jan_aligned = df_jan[df_final.columns]

    # concatena historial + fila(s) de enero‑20
    hist_dfs[tag] = pd.concat([df_final, df_jan_aligned],
                              ignore_index=True)

# ------------------------------------------------------------------
# viz rápida
# ------------------------------------------------------------------
# ------------------------------------------------------------------
# imprime SOLO los “nombres” (tags) disponibles
# ------------------------------------------------------------------
print("DataFrames creados →", ", ".join(hist_dfs.keys()))

# Elige qué modelo inspeccionar (cambia a gusto)
tag_show = "TUNED"          # por ejemplo: "BASE", "FINAL", "TWEED_SINLOG", …

# Si el tag no existe, muestra el primero disponible
if tag_show not in hist_dfs:
    tag_show = next(iter(hist_dfs))

print(f"\n► head() del histórico '{tag_show}'")
print(hist_dfs[tag_show].head())

print(f"\n► tail() del histórico '{tag_show}'")
print(hist_dfs[tag_show].tail())

# ------------------------------------------------------------------
# elimina 'fecha_primera_aparicion' en todos los históricos
# ------------------------------------------------------------------
for tag in hist_dfs:
    if "fecha_primera_aparicion" in hist_dfs[tag].columns:
        hist_dfs[tag] = hist_dfs[tag].drop(columns="fecha_primera_aparicion")

print("✅ Columna 'fecha_primera_aparicion' eliminada (donde existía).")

# vista rápida para comprobar
tag_show = "TUNED" if "TUNED" in hist_dfs else next(iter(hist_dfs))
print(f"\nhead() del histórico '{tag_show}' (sin la columna):")
print(hist_dfs[tag_show].head())
print(f"\ntail() del histórico '{tag_show}':")
print(hist_dfs[tag_show].tail())

"""Febrero Predicciones para cada modelo"""

def predict_next_month(
        df_hist: pd.DataFrame,          # histórico hasta MES‑1
        next_month: str,                # "2020-02-01"
        booster,                        # modelo a usar
        sinlog: bool,                   # escala (True = ya en tn)
        feature_pipeline,
        stock_tbl=None,                 # tabla de stocks
        cat_levels=None,                # dict niveles train
        drop_cols=DROP_COLS_NOW):
    """
    Devuelve SOLO las filas de next_month (formato original) con:
      • pred_tn    • y_pred_log
      y la columna tn sobrescrita por la predicción.
    """
    # ── 1) prototipo vacío del nuevo mes ──────────────────────────
    prev_month = (
        pd.to_datetime(next_month)
          - pd.offsets.MonthBegin()
    ).strftime("%Y-%m-01")
    proto = (
        df_hist[df_hist["periodo"] == prev_month]
          .drop(columns=["cust_request_tn"], errors="ignore")
          .assign(
              periodo=pd.to_datetime(next_month),
              tn=np.nan,
              cust_request_tn=np.nan,
              antiguedad_meses=lambda d: d["antiguedad_meses"] + 1
          )
    )
    df_ext = pd.concat([df_hist, proto], ignore_index=True)

    # ── 2) merge con stock desplazado +1 mes (opcional) ──────────
    if stock_tbl is not None and "stock_t0" in df_hist.columns:
        stock = (
            stock_tbl.copy()
            .assign(periodo=lambda d:
                pd.to_datetime(d["periodo"].astype(str), format="%Y%m")
                  + pd.offsets.MonthEnd(0)
                  + pd.offsets.MonthBegin(1)
            )
            .rename(columns={"stock_final": "stock_t0"})
        )
        df_ext = df_ext.merge(
            stock[["product_id", "periodo", "stock_t0"]],
            on=["product_id", "periodo"], how="left"
        )
        # completa NaN con promedio histórico del mismo mes
        if df_ext["stock_t0"].isna().any():
            month_num = pd.to_datetime(next_month).month
            avg_stock = (
                df_hist[df_hist["periodo"].dt.month == month_num]
                .groupby("product_id")["stock_t0"]
                .mean()
            )
            mask_new = df_ext["periodo"] == pd.to_datetime(next_month)
            df_ext.loc[mask_new, "stock_t0"] = (
                df_ext.loc[mask_new, "product_id"].map(avg_stock)
            )

    # ── 3) feature engineering + codificación ────────────────────
    df_feats = feature_pipeline(df_ext)
    # Usamos la nueva firma: solo df y cat_levels
    df_num   = encode_cats(df_feats, cat_levels)

    # ── 4) selecciona filas del nuevo mes y prepara matriz X ─────
    mask_new = df_num["periodo"] == pd.to_datetime(next_month)
    X_base   = df_num.loc[mask_new].drop(columns=drop_cols, errors="ignore")

    feat_used = booster.feature_name()
    if set(feat_used).issubset(X_base.columns):
        X_mat = X_base[feat_used].to_numpy(np.float32)
    else:
        X_mat = X_base.iloc[:, :len(feat_used)].to_numpy(np.float32)

    # ── 5) predicción y conversión a tn ──────────────────────────
    raw     = booster.predict(X_mat, num_iteration=booster.best_iteration)
    pred_tn = raw if sinlog else np.expm1(np.clip(raw, -20, 20))

    # ── 6) inserta resultados en df_ext y devuelve solo el mes ───
    df_ext.loc[mask_new, "y_pred_log"] = (
        np.log1p(pred_tn) if sinlog else raw
    )
    df_ext.loc[mask_new, "pred_tn"]    = pred_tn
    df_ext.loc[mask_new, "tn"]         = pred_tn

    return df_ext.loc[mask_new].reset_index(drop=True)



import numpy as np
import pandas as pd
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error

def december_error_df_safe(test_dec_num, booster, sinlog, tag):
    """
    Igual que december_error_df, pero:
     - Garantiza num_iteration válido
     - Filtra NaN/inf en predicciones
     - Chequea shapes y columnas clave
     - Devuelve, por modelo:
         product_id, real_tn,
         pred_tn_<tag>,
         abs_err_<tag>,
         mae_<tag>   <-- añadido
    """
    # 1) Features del modelo
    feat_used = booster.feature_name()

    # 2) Montar la matriz de predicción
    if set(feat_used).issubset(test_dec_num.columns):
        X_part = test_dec_num[feat_used]
    else:
        X_part = test_dec_num.iloc[:, :len(feat_used)]
    X_mat = X_part.to_numpy(dtype=np.float32)

    # 3) num_iteration seguro
    nit = booster.best_iteration or getattr(booster, "current_iteration", None)() or 1

    # 4) Raw preds
    raw = booster.predict(X_mat, num_iteration=nit)

    # 5) Transformación con clipping
    if sinlog:
        pred = raw
    else:
        raw_clipped = np.clip(raw, -20, 20)
        pred = np.expm1(raw_clipped)

    # 6) Sanitizar NaN / inf
    pred = np.asarray(pred, dtype=np.float64)
    mask_bad = ~np.isfinite(pred)
    if mask_bad.any():
        fill = np.median(pred[np.isfinite(pred)]) if np.isfinite(pred).any() else 0.0
        pred[mask_bad] = fill

    # 7) Check tamaño
    n_test = len(test_dec_num)
    if len(pred) != n_test:
        raise ValueError(f"[{tag}] Longitud pred ({len(pred)}) ≠ filas test ({n_test})")

    # 8) Validación de columnas clave
    if "product_id" not in test_dec_num.columns:
        raise KeyError("`product_id` no existe en test_dec_num")
    if "tn" not in test_dec_num.columns:
        raise KeyError("`tn` (real_tn) no existe en test_dec_num")

    # 9) Construir DataFrame de salida
    col_pred = f"pred_tn_{tag}"
    col_abs  = f"abs_err_{tag}"
    col_mae  = f"mae_{tag}"

    out = pd.DataFrame({
        "product_id":  test_dec_num["product_id"].values,
        col_pred:      pred,
        "real_tn":     test_dec_num["tn"].values,
    })

    # 10) Métricas fila a fila
    out[col_abs] = np.abs(out["real_tn"] - out[col_pred])
    # MAE = promedio de abs error, pero por fila es igual a abs error
    out[col_mae] = out[col_abs]

    return out

# ╔══════════════════════════════════════════════════════════╗
# 1) BUCLE  ▶  FEB‑20 para todos los modelos
# ╚══════════════════════════════════════════════════════════╝
FEB = "2020-02-01"

dec_errors_list = []
feb_submissions = []
hist_dfs_feb    = {}                 # histórico completo hasta FEB‑20

for tag, booster, sinlog in model_specs:
    df_hist = hist_dfs[tag]          # histórico hasta ENE‑20

    # predice febrero y concatena
    df_hist_feb_rows = predict_next_month(
        df_hist, FEB, booster, sinlog,
        feature_pipeline, df_tb_stocks_02, cat_levels
    )
    hist_dfs_feb[tag] = pd.concat([df_hist, df_hist_feb_rows], ignore_index=True)

    # CSV por modelo
    df_submit = df_hist_feb_rows[["product_id", "pred_tn"]] \
                   .rename(columns={"pred_tn": "tn"})
    csv_path  = f"/content/sample_data/df_kaggle_febrero_{tag}.csv"
    df_submit.to_csv(csv_path, index=False)
    print(f"✓ CSV febrero guardado: {csv_path}")

    feb_submissions.append(df_submit.rename(columns={"tn": f"pred_feb_{tag}"}))

    # error diciembre‑19
    test_dec_num = encode_cats(test_dec.copy(), cat_levels)
    dec_errors_list.append(december_error_df(test_dec_num, booster, sinlog, tag))

print("\n✅ Históricos extendidos hasta FEB‑20 generados.\n")

from functools import reduce

# -----------------------------------------------------------
# 1) Construye una lista de DataFrames solo con product_id y pred_feb_<TAG>
# -----------------------------------------------------------
dfs = []
for feb_item in feb_submissions:
    # feb_item puede ser (tag, df) o directamente df
    if isinstance(feb_item, tuple):
        tag, df_feb = feb_item
    else:
        # si es un DataFrame, extraemos el tag del nombre de la columna
        df_feb = feb_item
        col = [c for c in df_feb.columns if c.startswith("pred_feb_")][0]
        tag = col.replace("pred_feb_", "")

    # selecionamos las dos columnas que necesitamos
    dfs.append(df_feb[["product_id", f"pred_feb_{tag}"]])

# -----------------------------------------------------------
# 2) Merging all into a single wide DataFrame (one column per model)
# -----------------------------------------------------------
df_feb_all = reduce(
    lambda left, right: left.merge(right, on="product_id", how="outer"),
    dfs
)

print("▶ Submissions Febrero‑20 (one column per model):")
print(df_feb_all.head())

# (Opcional) exporta
# df_feb_all.to_csv("/content/sample_data/feb_submissions_wide.csv", index=False)

# -----------------------------------------------------------
# 0)  VARIABLES NECESARIAS YA EXISTEN EN TU ENTORNO
#     · cat_levels          (dict de niveles del TRAIN)
#     · cat_feats           (lista de columnas categóricas)
#     · DROP_COLS_NOW       (columnas a ignorar)
#     · test_dec            (diciembre‑19 original)
#     · base_model, tuned_model, tweed_opt_model, final_model
#       y sus versiones *_sinLog
# -----------------------------------------------------------


# -----------------------------------------------------------
# 1)  Elige si quieres UN modelo o TODOS
# -----------------------------------------------------------
chosen_tag = "BASE_SINLOG"           # ⇦ cámbialo si quieres otro (TUNED, etc.)
chosen_model = globals()[
    f"{'base_model' if 'BASE' in chosen_tag else 'tuned_model' if 'TUNED' in chosen_tag else 'tweed_opt_model'}"
    + ("_sinLog" if 'SINLOG' in chosen_tag else '')
]
sinlog_flag = 'SINLOG' in chosen_tag

all_specs = [
    ("BASE",          base_model,              False),
    ("TUNED",         tuned_model,             False),
    ("TWEED",         tweed_opt_model,         False),
    ("FINAL",         final_model,             False),
    ("BASE_SINLOG",   base_model_sinLog,       True),
    ("TUNED_SINLOG",  tuned_model_sinLog,      True),
    ("TWEED_SINLOG",  tweed_opt_model_sinLog,  True),
]

# -----------------------------------------------------------
# 2)  Codifica X_test con los niveles del TRAIN
# -----------------------------------------------------------
X_test_enc = encode_cats(test_dec.copy(), cat_levels)     # ← 2 argumentos
X_test_enc.drop(columns=DROP_COLS_NOW, errors="ignore", inplace=True)

# -----------------------------------------------------------
# 3‑a)  Predicciones SOLO para chosen_model
# -----------------------------------------------------------
feat_used = chosen_model.feature_name()
if set(feat_used).issubset(X_test_enc.columns):
    X_mat = X_test_enc[feat_used].to_numpy(np.float32)
else:                                   # fallback: selecciona por posición
    X_mat = X_test_enc.iloc[:, :len(feat_used)].to_numpy(np.float32)

raw     = chosen_model.predict(X_mat, num_iteration=chosen_model.best_iteration)
pred_tn = raw if sinlog_flag else np.expm1(np.clip(raw, -20, 20))

df_dec_pred = pd.DataFrame({
    "product_id": test_dec["product_id"].values,
    "y_pred_tn":  pred_tn
})
print("▶ Predicciones (modelo elegido)\n", df_dec_pred.head(), "\n")

# -----------------------------------------------------------
# 3‑b)  Predicciones para TODOS los modelos
# -----------------------------------------------------------
out_cols = {
    "product_id": test_dec["product_id"].values,
    "real_tn":    test_dec["tn"].values
}

for tag, mdl, sinlog in all_specs:
    feat = mdl.feature_name()
    if set(feat).issubset(X_test_enc.columns):
        Xmat = X_test_enc[feat].to_numpy(np.float32)
    else:
        Xmat = X_test_enc.iloc[:, :len(feat)].to_numpy(np.float32)

    raw = mdl.predict(Xmat, num_iteration=mdl.best_iteration)
    out_cols[f"pred_tn_{tag}"] = raw if sinlog else np.expm1(np.clip(raw, -20, 20))

df_dec_all = pd.DataFrame(out_cols)
print("▶ Predicciones diciembre‑19 (todos los modelos)\n", df_dec_all.head())
# df_dec_pred.to_csv(f"pred_dic19_{chosen_tag}.csv", index=False)
# df_dec_all.to_csv("pred_dic19_todos_modelos.csv", index=False)

df_dec_all.head()

# Partimos de tus DataFrames ya creados:
#  · df_dec_all  (product_id, real_tn, pred_tn_…)
#  · df_feb_all  (product_id, pred_feb_BASE, pred_feb_TUNED, …)

# 1) Fusionamos por product_id (left para conservar todo diciembre)
df_merged = df_dec_all.merge(
    df_feb_all,
    on="product_id",
    how="left"
)

# 2) Comprobación rápida
print("▶ df_dec_all con feb appended — shape:", df_merged.shape)
print(df_merged.head())

# 3) (Opcional) Exporta a CSV
out_path = "/content/sample_data/dec_y_feb_todos_modelos.csv"
df_merged.to_csv(out_path, index=False)
print("✅ Guardado CSV combinado en:", out_path)

df_mega = df_dec_all.merge(
    df_feb_all,
    on="product_id",
    how="outer",
    suffixes=("_dic", "_feb")
)

cols_feb = ["product_id"] + [c for c in df_mega.columns if c.endswith("_feb")]
df_feb_only = df_mega[cols_feb]
print(df_feb_only.head())
# df_feb_only.to_csv("pred_feb20_only.csv", index=False)

import pandas as pd
import numpy as np

def ensemble_with_fallback(
    model_specs: list[tuple[str, any, bool]],
    readyforModel: pd.DataFrame,
    df_dec_all: pd.DataFrame,
    df_feb_all: pd.DataFrame,
    start_hist: str = "2019-09-01",
    end_hist:   str = "2019-12-01",
    threshold:  float = 1.4,
    output_path_simple: str = "/content/sample_data/df_kaggle_febrero_ensemble.csv",
    output_path_full:   str = "/content/sample_data/df_kaggle_febrero_ensemble_full.csv"
) -> pd.DataFrame:
    """
    Construye un ensemble para febrero‑20 eligiendo:
      – el mejor modelo (en dic‑19, menor error absoluto)
      – salvo que su predicción feb sea > threshold * hist4_avg,
        en cuyo caso se prueba el segundo mejor,
        y si también está fuera de rango, se usa hist4_avg.
    Devuelve el DataFrame completo con métricas y decisiones.
    """
    # 1) promedio histórico últimos 4 meses
    df = readyforModel.copy()
    df["periodo"] = pd.to_datetime(df["periodo"])
    mask = (df["periodo"] >= pd.to_datetime(start_hist)) & \
           (df["periodo"] <= pd.to_datetime(end_hist))
    hist4_avg = df.loc[mask].groupby("product_id")["tn"].mean()

    # 2) prepara df_dec_all con errores absolutos
    dec = df_dec_all.set_index("product_id").copy()
    # columnas predicción de dic
    pred_dec_cols = [c for c in dec.columns if c.startswith("pred_tn_")]
    for c in pred_dec_cols:
        dec[f"abs_err_{c.replace('pred_tn_','')}"] = \
            (dec[c] - dec["real_tn"]).abs()
    # 3) define best y second según error en dic
    err_cols = [c for c in dec.columns if c.startswith("abs_err_")]
    errors_sorted = dec[err_cols] \
                        .apply(lambda row: row.sort_values().index.tolist(),
                               axis=1)
    best_models   = errors_sorted.apply(lambda lst: lst[0].replace("abs_err_",""))
    second_models = errors_sorted.apply(lambda lst: lst[1].replace("abs_err_",""))

    # 4) bucle de ensemble: recorre product_id
    feb = df_feb_all.set_index("product_id").copy()
    final_preds = {}
    final_choice = {}

    for pid in feb.index:
        avg4  = hist4_avg.get(pid, np.nan)
        best  = best_models.loc[pid]
        pbest = feb.at[pid, f"pred_feb_{best}"]

        # si pbest desborda el umbral
        if not np.isnan(avg4) and pbest > threshold * avg4:
            second  = second_models.loc[pid]
            psecond = feb.at[pid, f"pred_feb_{second}"]
            # si psecond también desborda → hist4_avg
            if not np.isnan(psecond) and psecond > threshold * avg4:
                final, choice = avg4, "HIST_AVG"
            else:
                final, choice = psecond, second
        else:
            final, choice = pbest, best

        final_preds[pid]  = final
        final_choice[pid] = choice

    # 5) arma df_simple (solo tn)
    df_simple = pd.DataFrame({
        "product_id": feb.index,
        "tn":         pd.Series(final_preds)
    })
    df_simple.to_csv(output_path_simple, index=False)
    print(f"🚀 CSV simple guardado en: {output_path_simple}")

    # 6) arma full_df uniendo todo
    full_df = dec.join(
        hist4_avg.rename("hist4_avg"), how="left"
    ).join(
        feb[[f"pred_feb_{tag}" for tag,_,_ in model_specs]], how="left"
    )
    full_df["chosen_model"] = pd.Series(final_choice)
    full_df["tn"]           = pd.Series(final_preds)
    full_df = full_df.reset_index()

    full_df.to_csv(output_path_full, index=False)
    print(f"✅ CSV completo guardado en: {output_path_full}")

    return full_df

full_ensemble = ensemble_with_fallback(
    model_specs,
    readyforModel,
    df_dec_all,    # DataFrame con: product_id, real_tn, pred_tn_<TAG>…
    df_feb_all,    # DataFrame con: product_id, pred_feb_<TAG>…
    start_hist="2019-09-01",
    end_hist="2019-12-01",
    threshold=1.4,
    output_path_simple="/content/sample_data/ensemble_simple.csv",
    output_path_full="/content/sample_data/ensemble_full.csv"
)

"""Predicciones puntuales - el modelo final sin log me falla"""